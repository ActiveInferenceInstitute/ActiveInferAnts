* Includes the mathematical framework for describing systems. 
* Set theory & Graph theory. 
* The chapter is about languages for the systems (math, natural language, visual, etc.). 
  * Math description is the basis of formal definition. So there can be alignment of shared reference frame for e.g. ‚Äúcomponent‚Äù. 
  * Relations among the modalities, gives more holistic understandings of systems/systemsness. 
  * Then can communicate and translate among/on/in those spaces; improved (understanding of) bandwidth.  
  * In natural language, always the questions of Lexical Semantics. In order to make sure were sharing the same kind of information. 
* J. Franklin, Aristotle, reality, mathematics
  * ‚ÄúAn Aristotelian Realist Philosophy of Mathematics: Mathematics as the Science of Quantity and Structure‚Äù
  * Linking to real things; numbers/geometries of things. Simpler maths. Then higher maths build on top. 
  * Franklin talks about the units of Things out there. How does the cognitive/conscious system do visual thing identification/clustering/typing/etc. How about the distinct objects in/out there? 
  * Identification of e.g. depth of field, Compositionality, conformance/deviance to a norm or standard. 
  * Natural & Social Sciences ‚Äî some of the domains like Information/Complexity/Systems are the Formal science (includes Math). 
  * https://www.sup.org/books/title/?id=2003 Homo Sacer, Agamben
* 20 marbles in the set ‚Äî intuitive natural language aspects of set/category theory.
* Boundaries & to the above
  * Boundaries (modeled on Bayesian graph as Markov Blanket). 
  * As one is perceiving sensory patterns (e.g. correlated movement of pixels) in light of embodied/learned reference frames (e.g. depth perception) ‚Äî posit that there are Neural circuits that do primary visual activities like Edge, Gradient, On-Off/Off-On, etc.
    * So for some low-level features there may be more canalization of the visual/object identification. The information flows forward, has articulation/factorization with the What and Where dorsal/ventral stream in the mammalian brain. 
  * Object/Entity of primary ontological status. 
  * Bottom layer of the neural net, direct connection to Observation, higher levels of the Neural net / Bayes graph / etc. ‚Üí higher-order representations. Abstraction. 
  * What about Sequences, time-as-process, imperatives/directives, non/propositional knowledge. 
    * Some on sequencing is on 1st and this book. 
    * Pre-frontal (hierarchical nesting)
    * Cerebellum, encoded programs/procedures in what happens next (B matrix / causal / transition model). 
  * Dynamics, Initial conditions, Boundary conditions; System
* Different areas of maths, different axiomatic bases. 
  * Any given axiom set constrains to within that math area. The instruments/tools. Different fields of math, back to Franklin ‚Äî people are good at small numbers of things. Coarse-graining and intuitions. Different types of infinity. 
  * The formal theories/methods can be good to do. 
  * Self-Identifying objects.... Higher/alternative dimensions
  * Operational closure ‚Äî Autopoiesis, Kant, Organismality.
    * Set of frames ‚Äî have we got all the frames arising in the universe were discussing. 
    * Statistical: Sampling from stationary? 
    * https://www.jstor.org/stable/26370801#: ‚ÄúKant‚Äôs Concept of Organism Revisited‚Äù
    * Identification of Space and Time 
* Formal is just one aspect of Systems Science 
  * Cognitive as only one step/aspect of the OODA loop. Still Action in reality, anchors back to 
* ‚ÄúThe Origin of Concepts‚Äù, Susan Carey (2009), Oxford
  * Where do the concepts come from; how are they formed. 
  * There is innate aspect & learned. 
  * Taking numbers, and patterns. 
* Both of George‚Äôs books discuss Concepts as systems instantiated in neural networks; how they are formed and learned at different levels in the hierarchy. 
  * Fido the üê∂, Mammals as a class. Similar across instances. 
* Part 3 in current book (no promises!) ‚Äî Complex Adaptive Evolvable Systems. 
  * Living Systems theory + etc. ‚Äî tie to cognition & eventually to consciousness.
  * Can see a living thing, human, as complex system with new cognitive structures giving rise to new/different enacted behaviors. 
* Cognitive Science & Systems Science
  * Relationship between computations (e.g. in the digital, neural, etc) ‚Äî Information Processing. 
  * Chris Fields 2023 course. 
  * Understanding what Information is, & what it is not. Information & Knowledge. 
  * See how the brain is processing these concepts, and treat it as CAES Archetype model ‚Äî all the component that need to be in any CAES. The forms will differ though will be there. That schema guides how/where to look for the brain. 
  * For example governance and strategic management. Consider the prefrontal cortex discussion above ‚Äî organizer of strategic manipulation of the concepts and models that we already have. What if in the distant future, this or that happens. 
  * Cognition in the human brain is natural. Look at the details. 
* General Systems Science 
  * Cognitive Science are a viewpoint/perspective. Other frames are needed.
  * Systems Science is both Ontology and Epistemology. 
  * An expanded Cognitive Science into e.g. Concepts
  * 

---



Chapter 4 

A Model of System and a Language to Represent It

* We propose a mathematically based definition of system based on the ontological framework developed in the last chapter. Finally, we use the semantics, syntax, and lexical elements from the ontology and the model of the human systems to propose a formal (but extensible) language of system.
* 4.1 In This Chapter
  * In the last chapter, we developed a set of terms and relations representing the elements of systemness. It was shown how systems come into existence in the real universe by a process of ontogenesis that started with the Big Bang and fundamental particles (matter-like entities) and forces (energy-like) mediated by information and constructing structures that encode knowledge about the history of things. That process did not stop with, for example, the creation of atoms. Indeed, we suspect it continues today so long as free energy is available to do the work needed for construction.
  * In this chapter, we turn to the use of ontology in devising the one key tool we need in order to express all of the elements of systemness, a language of systems.
  * This language is very special as a tool for communications between humans, and between humans and machines. That is, the language of systems possesses both the characteristics of a natural language for expressing systems and the characteristics of a formal language (both mathematics and computation) making it useful in constructing computer models for simulation of the systems we are gaining deep understanding about.
  * That is, the language we use for thought is based on systemness. This includes native or innate concepts and built-in mental computations enacting the constructs of systems and their behaviors.
  * The systemese hypothesis leads us to the idea that our natural languages are extensions of the fundamental elements.
  * Once we have a clear idea of how language works to communicate ideas about systemness we turn to a more direct way to use the ontology
* 4.2 The Communication of Ideas
  * Any attempt to produce a language of systems needs to be based on our understanding of human cognition and the nature of public language. Our goal is to link how people communicate ideas about the world to one another and the ability to describe the structures and functions of systems. Additionally, our goal is to formalize the language in such a way that humans can communicate with computers (and vice versa) effciently and effectively. So, our frst task is to consider how ideas are communicated.
  * Ideas are mental states that involve the composition of several concepts into single modules of thought. Sentences spoken in public languages express ideas.
  * Time and space do not permit a full accounting of mentation here. What follows is a brief outline of the major aspects as related to conscious thought and language.
* 4.2.1 Representations of Concepts, Beliefs, and Ideas
* 4.2.1.1 Concepts
  * The mind holds units of thought that correspond with things in the world and we call them concepts. Cognitive scientists and linguists have studied how human beings possess, construct, or form concepts through learning processes, and how they then use concepts to compose ideas that can be shared through language with other humans.
* 4.2.1.1.1 Conceptual Hierarchies
* 4.2.1.1.2 Innate vs. Learned Concepts
  * As mentioned above, human beings, and presumably many other kinds of animals, come with built-in, or innate concepts of the most important things in the world that they will need to interact with from an early point in their development. Some of these, such as the ‚Äúobject‚Äù concept mentioned above, are already at work in very young infants. There is evidence that the concepts of ‚Äúcausality,‚Äù ‚Äúagent,‚Äù ‚Äúagency,‚Äù and ‚Äúintentionality‚Äù are at work at an early age as well (cf., Carey 2009).
  * What we don‚Äôt come with is specific concepts of particular objects and agents. These have to be constructed through experience with the world.
  * And they learn the rules for composing these concepts into ideas, the syntax of their native language.
* 4.2.1.2 Ideas
  * Sentences such as ‚ÄúBob believes Jane believes he likes her,‚Äù express how one person can possess a model/concept of another person‚Äôs mind, called ‚Äútheory of mind,‚Äù or ‚Äúfolk psychology‚Äù in the cognitive science literature (recall Principle #9 in Chap. 2). That sentence also reveals another important aspect of human cognition. The use of the word ‚Äúbelieves‚Äù refects on the fact that people are not just conscious (of things in the world) but of their own thoughts (recall Principle #10). They are ‚Äúself-conscious.‚Äù
* 4.2.1.3 Beliefs
  * We will not have a great deal to say about beliefs (as opposed to concepts) other than to recognize that what one believes about the nature of a conceptual object or agent is, or can be, quite idiosyncratic, path dependent on the sequences of experiences one has with the subjects of concepts
* 4.2.2 Names of Things‚ÄîAbstraction and Language
  * Names of things, relations, and actions are special abstract concepts. They are formed in auditory processing space and are composed of phonemes produced by auditory feature detectors.
  * Words are abstract and mostly arbitrary representations in the brain. But they are associated, through learning, with the actual iconic image of the concept representation in memory. Humans can create other abstract symbols, letters to represent phonemes (or combinations thereof) and written sequences of letters to represent the words.
* 4.2.3 Language of Thought and Innate Concepts.
  * Figure 4.1 image.png
  * The use of a LOT engine for production of complex concepts starts with the innate archetype concepts discussed above, even before it is used to connect those concepts as ideas (and possibly beliefs).
  * As originally conceived, LOT is a computational theory of mind (CMT) where computation is taken to be an algorithmic manipulation of strings of ‚Äúsymbols‚Äù (something Fodor seemed to have both asserted and denied!) That neural representations of concepts (things in the world) are very clearly consistent subnetworks of fring patterns constructed either from innate concepts or from learned ones, they can be construed as being symbol-like (cf., Abdou et al. 2018; Brodt et al. 2018). That there is some internal syntax for combining these symbol-like fring patterns into composite patterns in a kind of sequential stimulation, one can say that the brain does indeed compute an internal language to produce constructs that make sense, that is, have semantic content and fulfll pragmatics.
* 4.2.4 The Systemese Hypothesis
  * This hypothesis may be considered speculative, of course. But the existence of some kind of LOT along with an examination of a few of the known innate concepts lends strong support for the idea that LOT is system syntax and those core concepts are system archetypes.
  * Our immediate interest in the systemese hypothesis is in devising a public language (using English names of things, though this is not required) of systems that is based on the ontology framework and embodies the systems structure/function syntax so that human beings can describe systems and their behaviors to other humans and to machines. Both humans and machines (computers) can form representations of the described systems and achieve a more consistent communication between systems in the world, systems in the mind, systems in the abstract, and systems in computer code.
  * That language must include an innate representation of containment and a model of a generic or archetypical container (as mentioned above). It includes an attribute of capacity, which is a variable. Now when a child encounters a real container in the world, they do not have to wonder what on earth it is, their archetype model provides the answer‚Äîthe syntax and the semantics. The brain builds a higher-level archetype using the sensory attributes of the particular container.
  * In sum, then, systemese consists of a set of innate archetype concepts (models) that evolved to reflect the systems (and components of systems) that exist in the environments of animals with which they have to interact. 
    * These concepts come with built-in syntactical rules for combination that preserve the semantics and pragmatics of the situations in the world. All animals with brains of any complexity ‚Äúthink‚Äù in their version of systemese, and this is not a conscious process. 
    * Humans, on the other hand, have an extraordinarily complex environment‚Äîcomplex systems with which to interact‚Äîand accordingly have a much more sophisticated set of innate concepts with which to construct more complex thoughts. Moreover, humans possess a supervening concept encoding mechanism that associates specific auditory patterns (speech) with complex constructions, and these become the names given to things, relations, and actions‚Äîlanguage.
* 4.3 A Formal Definition of System
  * We are now ready to develop a formal definition of a system that, along with the ontological commitments of the last chapter, will be the basis for producing a language for systems. This language will be used to guide analysis of systems, since the definition tells us what we should be looking for, and to build models of systems at various levels of abstraction.
    * There is a point we need to be clear about. What is being presented here is, itself, a concept about what a system consists of, how it is composed. It is not being represented as the ‚Äúgeneral theory of systems,‚Äù though it might be a candidate for that title. It is based on having made the ontological commitments from Chap. 3 and following them to their ‚Äúlogical‚Äù conclusions. That being said, we are fairly certain that if there is a general systems theory, as posited by von Bertalanffy, for example, then it is likely to look something like this. Many theorists have attempted to define systemness in a formal way and suggested that their definition constituted the general theory. Some of them have shared common approaches (e.g., using set theoretical language) yet after nearly six decades, no universal agreement over what exactly ‚Äúsystem‚Äù means has emerged in the scientific literature. The reader may recall from the discussion in the Preface regarding the way we ‚Äútalk‚Äù about systems science that we regard the latter as more of a meta-science than just another kind of science. Which means that systems science theories are not ordinary theories at all, but metaphysical theories. Ergo, any definition of system must simply accept some ontological (and epistemological) commitments and then get on with it. Only in successes with usage over an extended time will the veracity (or acceptance) of a definition be turned into a meta-theory, i.e., a general theory of systems.
  * The definition is given in three complimentary (should be complementary?) forms: verbal, graphical, and mathematical. All three forms provide views of the system definition that provide access to stakeholders from different backgrounds. The mathematical defnition is needed in order to create an abstract representation of the system defnition that can be directly applied to creating a language of system (hereafter called SL)
* 4.3.1 Verbal
  * The lexicon of SL is taken from the primitive and derived elements in Chap. 3. All verbal descriptions use those lexical elements as the skeleton of meaningful statements. For example, in describing a subsystem that processes material inputs (with energy) to produce a product output we would simply say something like: ‚ÄúProcess M takes in materials A and B from sources 1 and 2 along with energy E from source 3 to make product Z with waste product X going to sinks 5 and 6 respectively, at an effciency of 68%.‚Äù
  * The verbal descriptions start with the lexical elements acting as placeholders for specifc items, such as material A is a placeholder for ‚Äúwater‚Äù in the above example. With each additional statement, the system description gets refned. As the deconstruction of the system proceeds, sub-paragraphs are added, each describing the subsystem at a lower level of detail.
  * Figure 4.2 image.png
* 4.3.2 Graphical
  * Following the age-old dictum that ‚ÄúA picture is worth a thousand words,‚Äù SL provides a graphical way to express systems. This has actually been done in most modeling languages such as Stella (for system dynamics), and SysML. Various kinds of diagrams can be produced to capture the system as a model. Figure 4.3 provides a graphical version of the system paragraph from above.
    * Figure 4.3 image.png
* 4.3.3 A Mathematical Structure Defining a System
  * A formal language is based on the existence of a formal structure into which elements of the language ft (see Pragmatics section below). For example, a computer programming language is based on a formal structure involving arithmetic, logic, and conditional fow control (e.g., IF-THEN) used to describe data processing algorithms. These are realized in an actual computer architecture based on the theory of computation (e.g., the Turing Machine formalism and the von Neumann architecture).
  * Deriving a definition of system from the principles and the ontology, a system S is a 7-tuple:
    * image.png
  * where i and l are indexes. The entire tuple, and essentially all tuples in equations, should have <> brackets, Set notation. The index i is a subsystem index and the index l is the level of organization in the system-subsystem hierarchy. Both are 0 for the initial system of interest, S0,0 is the designated SOI. Recall from the previous chapter that the ontological framework specifed that the SOI be designated as level 0. This will make sense in this defnition as we show how the system defnition leads to a natural way to deconstruct the component/interaction level (+1) in the framework. Several later chapters in the book will describe the application of these equations (and the knowledgebase described in Chap. 8) to even more complex adaptive and evolvable systems (CAESs) described in Part III.
  * image.png
  * Figure 4.4 image.png
  * Figure 4.5 image.png
  * Figure 4.6 image.png
* 4.3.3.2 Interactions
* 4.3.3.2.1 Between Components Internally
  * Figure 4.7 image.png
* 4.3.3.2.2 Between Environment and Components of S
  * Figure 4.8 image.png
* 4.3.3.3 Boundary
  * Up to this point the definition of system should resemble those given in most accounts whether in mathematical form or not. Languages for modeling systems, such as system dynamics (SD) utilize the same sets of elements as described so far but not in this mathematical form. At this point we start to depart from historical accounts of system defnitions as will be explained along the way. The frst departure involves the concept of a boundary. If you recall from the last chapter, we claimed that boundedness is a real element (has frst-class ontological status). Boundedness constructs effective boundaries to systems
  * For purposes of analysis and design, we will make boundaries explicit elements of a system defnition. The boundary, B in Eq. 4.1, at level l, then is a tuple. That is:
    * Figure 4.9image.png
    * Essentially complete mapping to the ‚ÄúThing‚Äù of Free Energy Principle, these are complementary approaches. 
    * Figure 4.10 image.png
    * Figure 4.11 image.png
    * Figure 4.12 image.png
    * Figure 4.13 image.png
    * Figure 4.14 image.png
    * Figure 4.15 image.png
    * Figure 4.16 image.png
    * At present it includes such properties as porosity (0 being completely non-porous) and ‚Äúperceptive fuzziness,‚Äù meaning the degree to which it is easily perceived; 0 being able to identify and locate in space the separator between inside and outside any number greater than 0 but less than 1 being the degree to which a physical phenomenon corresponding to ‚Äúkeeping the insides in and the outsides out.‚Äù
      * This paper https://arxiv.org/abs/2207.07620 Weak Markov Blankets in High-Dimensional, Sparsely-Coupled Random Dynamical Systems (2022) ‚Äî introduced the ‚ÄúBlanket Index‚Äù, between 0 and 1, to describe exactly what is being suggested here. 
    * Figure 4.17 image.png
* As a rule of thumb, the interfaces for more complex systems tend to be more complex themselves. A simple opening in the wall of a hut functions as a door, but it allows anyone (or anything) to walk through.13 An actual hinged door, with door knobs inside and out, and an internal lock mechanism (keyed from the outside) in a modern (and more complex) structure is a bit more complex and has a controlled entry/exit protocol. Entries and exits from airport departure/arrival gates have become exceedingly complex with separate entry and exit paths required by security needs.
* 4.3.3.4 Transformations
  * This is, effectively, an abstract model of the transformation that will be refned as the deconstruction process continues. One of the advantages of following this framework is that the higher-level transformations get refned by discovery of the lower-level ones. Once more is known about the internal transformations of subsystems and their combined effects, the higher-level model can be made more rigorous (Principle 12 in Chap. 2). This recursive improvement tracks the deconstruction all the way down to the leaf nodes in the system tree.
  * When system engineers believe they are obtaining such a specifcation (e.g., in requirements gathering), they are often faced later with discontinuities between expectations and actualities that are highly disruptive to design projects. With the approach of deep analysis promoted in this book (top-down deconstruction of transformations and bottom-up refnement) the engineering process will be following a formal procedure that they often end up following informally anyway!
    * I believe this corresponds to a hierarchical predictive coding scheme, implemented sequentially via a cognitive agent or team, by their regime of attention to the system of interest. 
* 4.3.3.5 Memory
* 4.3.3.6 Time
* 4.3.3.6.1 Time Indexing
* 4.3.3.6.2 Cyclic Intervals
  * There are instances when the periodicity of a cycle may be varied in a constrained-random fashion, using, for example, the bounded Monte Carlo method.
  * In FEP we can use a sampling-based Monte Carlo approach, and/or a Variational Inference approach. 
* 4.3.3.7 Considering Very Complex Systems
  * Complexity is a very diffcult concept and the word itself has many different senses. Intuitively most people have the notion that if something has many parts (i.e., subsystems) and many interactions (i.e., fows) between the parts and many interactions with its environment, then it is complex. There are many different ways to characterize the complexity of a system. 
  * Here we investigate two complementary approaches.
    * The first looks at what we can consider ‚Äústructural‚Äù complexity using the above-mentioned intuition. 
    * The second approach looks at dynamic or ‚Äúbehavioral‚Äù complexity, that is, how are the behaviors of the complex system in relation to their environmental interactions. Both include consideration for the number of states that a system can be in, but behavioral complexity looks only at the states of interactions with the environment.
* 4.3.3.7.1 Simonian Complexity
  * There are two fundamental aspects that go into defning complexity: structural complexity and functional complexity. One deals with the hierarchy of modules and submodules and the other deals with what each module does. The former is exposed by the structural decomposition process outlined in Chap. 6. The latter is much more diffcult to estimate. For our purposes here, we will use the concept of a state space to approximate the metric of functional complexity.
* 4.3.3.7.2 Behavioral Complexity
  * Behavioral complexity can be far more diffcult to characterize compared with structural complexity. The behaviors of a system are related, clearly, to the number of states the system might be in as described above. However, there are possibilities of transitions from internal states that are not easily modeled by fnite state machines, even probabilistic machines. Among these possibilities is behavior resulting from internal non-linearities, resulting, for example, in chaotic behaviors. That is, if one captures time series data on the externally displayed states of a system, they describe a chaotic attractor basin in phase space.
* 4.3.3.8 Ontogenesis of Greater Complexity
  * We now turn attention to the problem of how to use our ontological framework, understanding of ontogenesis, and our formal framework for describing systemness to create a way to describe all systems, but particularly able to describe even the most complex systems we encounter. We need a language.
* 4.4 Toward a Language of Systems
  * Armed with the mathematical structure described in Eq. 4.1 and subsequent equations, along with the ontological commitments in Chap. 3, we are in a position to design a formal language of systems (SL). With such a language we will be able to describe, in principle, any system in any domain, for example, biological or sociological. The language uses generic terms to represent various elements of systemness, for example, nouns like ‚Äúprocess‚Äù to embody a system that does work and ‚Äúfow‚Äù to embody the movement of materials, energies, and messages. Its syntax is based on allowable patterns or relations, for example, a work process that varies a fow (e.g., a valve or resistor) cannot be put inside a stock; it has to be inserted into the fow.
* 4.4.1 Semantics
  * SL is a ‚Äúdescription‚Äù language. It is used to describe structures, relations, functions, and the other items in the ontology of Chap. 3. This is different from a typical programming language, which is designed to ‚Äúdescribe‚Äù algorithms, or sequences of steps in a data manipulation process. An algorithmic description language (i.e., programming language like JavaScript) is incorporated into SL to specify the operations of behaviors of the various elements, such as interface protocols. Relations between elements are built into the syntax (see below).
  * 4.4.1.1 Lexical Elements
  * 4.4.1.2 Descriptions
  * 4.4.1.2.1 Components
  * 4.4.1.2.2 Flows
  * 4.4.1.2.3 Behaviors
  * 4.4.1.3 Syntax
* 4.5 Example
  * As mentioned above, SL is a description language; it has more in common with document description languages like hypertext markup language (HTML) used to describe web pages for display in browsers than with conventional programming languages
  * . Combined with a scripting language like JavaScript, which is used to specify behaviors within the web page environment, a markup-like language is quite powerful in providing form and function in models. SL has much in common with HTML, or more correctly with the Extended Markup Language (XML), which is a superset of HTML.  Below we provide a very simplifed example of a system described in a markup language we‚Äôll refer to as sysXML to give some idea of how this computational platform can be used to do so. We should emphasize that this exercise is just a preliminary concept of how SL might be implemented and is as much a playful exploration as a serious example.
  * Figures 4.11, 4.12, 4.13 and 4.14 provide examples of a system of interest in SL graphics along with some of the captured data, that is, labels. Listings 4.1, 4.2, 4.3 and 4.4 show the sysXML output from the captured data as organized in the knowledgebase after analysis. The system is analyzed following the procedures given in Chap. 6, in a top-down decomposition with the analytical engine capturing the results into the knowledgebase format that will be described in more detail in Chap. 8. From there, it is possible to generate a systems dynamics-like simulation model and produce a human- (and machine-) readable sysXML specification as shown below.
* Listing 4.1 The XML output from the knowledgebase of the description of the Steel-Plant system. This shows the opening of the description (an SOI with id of S0) and its main environment (level-1) sources, sinks, and flows.
* XML code uses tag elements to denote the structural element being described. We have defned the tag ‚ÄúSOI‚Äù to represent the start of a system description document. Tags are demarcated by the ‚Äú<‚Äù and ‚Äú>‚Äù characters and the end of a description is given with the tags ‚Äú.‚Äù The other entities included within the brackets are called attributes. Here the majority of attributes are ‚Äúname,‚Äù ‚Äúid‚Äù (meaning the identifcation number in prefx dotted number format), ‚Äútype‚Äù and ‚Äúsubtype.‚Äù The latter two attributes may be present multiple times since elements might have more principle and subordinate characteristics that need to be identifed. The words in all capital letters are predefned attribute values that largely follow the ontology of the last chapter. We suspect that all readers will have no trouble interpreting the tags and their contents (which are for the most part simple strings, but also additional tags that are part of the content of superior tags‚Äîwhich are indented to show their subordination).
* Figure 4.14 represents what a user/analyst would see on a screen using the analytical engine we will describe in Chap. 6. They would drag elements to their positions and be prompted for the names/identifcations of the elements along with other attributes as shown in Table  4.1. Later, the system would produce the code in Listing 4.1.
* Listing 4.2 Boundary description (note the indentation level is the same as the ) tags.
* In Fig.  4.16 we begin the process of exposing the internal subsystems of the Steel-Plant SOI. In this depiction, we have identifed four internal subsystems of interest, three associated with the environment-boundary interfaces and one, the production shop that interacts with the three
* Listing 4.3 This listing does not show the whole system but does show how the components, fows, and other objects in Eq. 4.1 are represented.
* Listing 4.4 This listing shows how the subsystems are treated. In this listing we decompose the Iron-Inventory, identifed as C0.1 in Fig. 4.17.
* 4.6 Conclusion
  * In this chapter, we have covered a number of closely related topics surrounding the concept of a language of systems. We started by describing systems thinking. First, we considered the way in which some people can think explicitly about systemness. This has been the standard way of looking at it. But we then claimed that there is another way to look at thinking itself as implicit systems thinking. That is, our brains already communicate among various representational modules based on the kinds of systems constructs explored in the last chapter. Many linguistics researchers and philosophers of mind believe that every human brain has a subconscious language of thought that supports the symbolic module interchanges/integrations. We suggested that this is actually ‚Äúsystemese,‚Äù the mental language.
  * Now that we have a language, we turn to a somewhat more detailed examination of the entire understanding process showing how the various pieces operate and ft together. That will be the subject of Chap. 5.
* .
* By George Mobus