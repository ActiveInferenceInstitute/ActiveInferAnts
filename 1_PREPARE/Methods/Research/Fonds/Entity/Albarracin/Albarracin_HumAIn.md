https://www.youtube.com/watch?v=sxxqdksdQqM
HumAIn Dialogues: Mahault Albarracin - From Consciousness to Social Groups through Active Inference

This is particularly relevant in today's world where the increasing complexity and interconnectedness of social networks, institutions and global systems presents an urgent challenge that requires multiscale understanding and therefore also interdisciplinarity. It also speaks to a deeper question about the fundamental nature of reality and the relationship between different ontological levels. How do the low-level physical processes in the brain give rise to the high-level phenomenology of conscious experience? How do individual actions and perceptions construct shared social realities that in turn constrain individual behavior? Grappling with these questions requires us to confront the complex ways in which information and matter, the mental and the physical, the individual and the collective intertwine, co-determine each other and may not be as separate as we generally tend to think.

The quest to understand consciousness has led to the development of numerous theories and models, a few of which I will present today, each attempting to capture the essential properties of subjective experience. But the field of consciousness studies currently lacks a unifying framework that integrates these diverse perspectives. In this talk, my co-authors and I propose that the active inference framework, rooted in the Bayesian brain hypothesis, is a really promising path towards a minimum unifying model of consciousness. We will explore it in conjunction with the holographic screen theory and the concept of Markov blankets, and suggest that it gives rise to the inner screen theory of consciousness. We will try to demonstrate how this theory not only accounts for the temporal structure of consciousness but also sheds light on the formation of shared expectations and the emergence of epistemic communities in social networks. Then we explore how this modeling approach can be used to set the computational foundations for artificial intelligence agents interacting in ecosystems of intelligence. At the end, I will show you all the references to the different papers that I'm presenting and a few references that served for this presentation.

So let's start more broadly with the field of consciousness studies. As we said, it has produced a wide array of theories attempting to explain the nature and origins of subjective experience. For instance, Global Workspace Theory, proposed by Bernard Baars (I'm a little so proud of myself I've got a little animation and stuff), posits that consciousness arises when information is broadcast widely throughout the brain via a global workspace, making it available to various cognitive processes. The global workspace acts as a central hub for information sharing, allowing different specialized modules to communicate and coordinate their activities. But it doesn't directly address the hard problem of why and how global information sharing gives rise to subjective experience.

Integrated Information Theory, developed by Giulio Tononi, suggests that consciousness is equivalent to integrated information and that the amount of integrated information generated by a system determines the level or quality of consciousness. Despite its mathematical rigor, it has been criticized for leading to counterintuitive conclusions such as the possibility of consciousness in simple systems like photodiodes.

Higher Order Thought Theory, with its proponents such as David Rosenthal, argues that consciousness emerges when a higher-order mental state (a thought or perception) takes another mental state as its object. In other words, a mental state becomes conscious when it is represented by a higher-order thought. For example, a visual perception of a red apple becomes conscious when it is accompanied by a higher-order thought like "I see a red apple". It has been criticized for failing to capture the qualitative aspects of experience and for leading to an infinite regress of higher-order states.

Attention Schema Theory, developed by Michael Graziano in part, proposes that consciousness arises as a result of the brain's ability to model attention. According to AST, the brain constructs a simplified representation or schema of its own attentional processes, which gives rise to the subjective experience of awareness. It has been criticized for not addressing the hard problem of consciousness and for relying heavily on the concept of internal models, which some argue are not sufficient for explaining subjective experience.

Global Neuronal Workspace Theory, an extension of Global Workspace Theory, was proposed in part by Stanislas Dehaene and Jean-Pierre Changeux. This theory focuses on the neural mechanisms underlying conscious access and posits that conscious perception occurs when incoming sensory information is propagated from sensory cortices to a network of neurons with long-range connections, particularly in the prefrontal and parietal cortices. But it still faces the same challenges in explaining the subjective qualitative nature of consciousness.

Now another fan favorite is the Orchestrated Objective Reduction or Orch OR theory, proposed by Stuart Hameroff and Roger Penrose. It suggests that consciousness originates from quantum computations in microtubules inside neurons, rather than from neural activity patterns alone. The theory claims that microtubules can implement quantum computations and exist in a quantum superposition of multiple possible states. When a threshold is reached, the quantum state of microtubules undergoes objective reduction, collapsing into a single classical state, which is thought to correspond to a moment of conscious experience. Its critics argue that the theory lacks empirical support and relies on controversial assumptions about the role of quantum processes in the brain.

These theories, along with others like predictive processing or recurrent processing, have advanced our understanding of consciousness by highlighting key aspects such as information integration, higher-order representation, the role of attention, and the neural correlates of conscious access. But they have limitations in fully explaining the subjective qualitative nature of consciousness and bridging the explanatory gap between objective physical processes and first-person phenomenology. And current models, more to our point, do not really adequately address the question of how conscious experiences can be shared amongst individuals. This is a crucial aspect of human consciousness, as we regularly engage in acts of communication, empathy, collective meaning-making that seem to depend on shared experiential reality. Theories that focus solely on individual brains or abstract information processes struggle to account for the intersubjective and socially embedded nature of consciousness.

The limitations and challenges faced by existing models really underscore the need for a minimum unifying model of consciousness, as proposed by Wanja Wiese. Such a model would aim to integrate the key insights from various theories while remaining minimally committed to specific implementation details. By identifying the necessary properties of consciousness that are common across different theories, a MUM could provide a common foundation for future research and help bridge the gaps between different approaches.

Enter the Bayesian brain hypothesis, which has emerged as a unifying framework for understanding various aspects of brain function, including perception and cognition. This hypothesis posits that the brain operates as a Bayesian inference machine, constantly making predictions about the world and updating these predictions based on sensory evidence. It must deal with uncertainty in order to interact effectively with the environment. The origins of the Bayesian brain hypothesis can be traced back to the work of Hermann von Helmholtz in the 19th century, who proposed that perception involves unconscious inference. More recently, the concept of analysis by synthesis, put forward by Ulric Neisser and colleagues, suggested that the brain generates internal models of the world and compares them with sensory inputs to recognize patterns. Jerome Bruner and others also developed predictive coding theories of perception, which posit that the brain actively predicts sensory input and updates its predictions based on prediction errors.

Building on these ideas, Karl Friston has developed active inference, along with a slew of colleagues who joined him, which extends predictive coding principles to encompass action and learning. Active inference suggests that perception, action, and learning are all driven by the minimization of variational free energy, a measure of the discrepancy between the brain's predictions or beliefs and the actual sensory data. By minimizing free energy, the brain can update its beliefs about the world and generate adaptive behavior that brings its predictions closer to reality. The Free Energy Principle, which underlies all of this, has been proposed as a unifying theory of brain function and states that any self-organizing system, such as the brain, must minimize its free energy in order to maintain its structure and function over time.

So if we dive a little bit into active inference, we must talk about the generative model. The generative model is a probabilistic model that captures the agent's beliefs about the world, including the relations between hidden states, observations, and actions. It has several key components:

1. States: Hidden or latent variables that represent the underlying causes of sensory observations. States can be discrete or continuous and can represent various aspects of the environment or the agent itself, for instance, position of an object or the agent's own internal states.

2. Observations: Sensory data that the agent receives from the environment. Observations are assumed to be generated by the hidden states according to a likelihood function.

3. Actions: Interventions that the agent can make to influence the environment. Actions are selected based on their expected free energy, which balances the agent's goals (or their prior preferences) with its drive to minimize uncertainty.

4. Transitions: Probability distributions that describe how hidden states evolve over time given the agent's actions. Transitions capture the agent's beliefs about the dynamics of the environment.

5. Likelihoods: Probability distributions that specify how observations are generated from the hidden states. The likelihoods encode beliefs about the relationship between the states and the observations.

To minimize free energy, we can go through a combination of perception (updating beliefs about the hidden states based on observations) and actions (selecting the actions that are expected to minimize free energy). We can understand this process as a form of inference because it involves updating beliefs based on evidence, and it is active because the agent can take actions to shape its sensory input and reduce uncertainty.

The Bayesian brain hypothesis, and therefore active inference, are compatible with many of the leading theories of consciousness. For example, the global workspace in Global Workspace Theory can be interpreted as a high-level generative model that integrates information from multiple sensory modalities and cognitive processes. The process of broadcasting information to the global workspace can be seen as a form of Bayesian model selection, where the most informative and relevant predictions are selected for conscious processing.

Similarly, the concept of integrated information can be related to the complexity and accuracy of the brain's generative models. A system with high integrated information can be thought of as having a rich and well-calibrated generative model that can make precise predictions about a wide range of sensory data. The experience of consciousness may arise from the brain's ability to minimize free energy by refining its generative model through active inference.

Higher Order Thought Theory and the Attention Schema Theory can also be interpreted in terms of the Bayesian brain hypothesis. Higher Order Thought Theory can be seen as a meta-representation of the brain's own inference processes, allowing for self-monitoring and metacognition. The attention schema can be understood as a generative model of the brain's own attentional processes, which enables the control and allocation of cognitive resources.

But despite its compatibility with existing theories of consciousness, the Bayesian brain hypothesis does not directly address why certain types of information processes are accompanied by subjective experiences, not directly anyway. This is why we turn to the concept of a Markov blanket, which is central in active inference. A Markov blanket is a statistical boundary that separates an internal system (the brain) from its external environment. In active inference formulations, at least, it consists of two states:

1. Sensory states: Variables that represent the sensory data received by the agent, which are influenced by the external states but do not directly influence them.

2. Active states: Variables that represent the agent's actions, which influence the external states but are not directly influenced by them.

The sensory and active states together form a Markov blanket that induces a conditional independence between the internal states (the agent's beliefs) and the external states (the environment). This means that the agent's beliefs about the world are only influenced by the environment through the sensory states, and the agents can only influence the environment through its actions. This gives us a really principled way to demarcate the boundaries of the agent and to describe the interactions between the agent and its environment. The internal state of the agent can be understood as encoding the generative model of the world, which is used to predict sensory observations and to select actions that minimize free energy.

The concept of nested Markov blankets has also been used to describe the hierarchical organization of the brain, with each level of the hierarchy corresponding to a different spatial and temporal scale of information processing. In this view, the brain's generative model can be thought of as a hierarchy of nested Markov blankets, with higher levels encoding more abstract and long-term representations of the world, and lower levels encoding more concrete and short-term representations. This also offers us a principled way to investigate the emergence of subjective experience and the demarcation of the self by considering the dynamics of the brain's internal states and their interactions with the environment through the lens of free energy minimization.

So let's take a tiny detour. I did preempt this by saying we were going to talk about the holographic screen theory, and this is where if you have pointed questions, I will direct to the true geniuses, which are Chris Fields, Karl Friston, Leah Glazebrook, and Marcello Buiatti, which basically explain - well, they've created the holographic screen theory, and it helps us explain how information processing in the brain can be understood in terms of the Free Energy Principle and active inference, as it can be seen as a generalization of the Markov blanket construct.

The holographic screen theory takes the idea further by proposing that the boundary between any two mutually separable systems can be considered a holographic screen, which encodes all the classical information that can be exchanged between the systems. This is a direct consequence of the holographic principle, which states that the information content of a bulk can be, in principle, encoded on a lower-dimensional surface or boundary.

In the context of the brain, the holographic screen theory suggests that the boundaries between different levels of the brain's hierarchical generative models (between sensory and higher-order cortical areas, for instance) can be seen as holographic screens. These screens encode the classical information that is exchanged between the levels, allowing for the integration of information across scales and the emergence of coherent, unified representations of the self and the world.

A key insight of the holographic screen theory is that the information encoded on these screens is finite and discrete rather than mostly continuous. This is because the screens are composed of a finite number of sites or pixels, each of which can encode a finite amount of classical information (like one bit). The finiteness of the information implies that the brain's representations of the world are necessarily coarse-grained and subject to resolution limits.

It posits that the information encoded on the screen is subject to a cyclic, reciprocal process of writing and reading. Specifically, each level of the hierarchy alternatively writes information to the screen below it via descending predictions and reads information from the screen via ascending prediction errors. This process of writing and reading implements a form of active inference, where the brain actively updates its generative models to minimize prediction errors and maximize the evidence for its models.

The holographic screen theory also has important implications for the arrow of time and the nature of memory in the brain. Because the information encoded on the screen is finite and discrete, the process of writing and reading induces a local arrow of time that flows in opposite directions across the screen. This means that from the perspective of any given level in the hierarchy, the past is encoded on the screens below it (which it can read from), while the future is encoded on the screens above it (which it can write to). The theory suggests that memory is implemented by the process of writing information to the screen and then reading it back at a later time, which implies that memory is an active process of reconstruction that is shaped by the brain's current generative models and predictions.

To bring this all together, the inner screen theory of consciousness represents an integration of active inference and its Markov blankets, and the holographic screen theory. It posits that consciousness arises as an emergent property of the active inference processes that occur on the brain's hierarchically nested Markov blankets, which can be understood as holographic screens encoding classical information, mediated by the deployment of covert attention and precision weighting.

We propose that conscious experience corresponds to the winning hypothesis or model that best explains the current sensory data after all the relevant prediction errors have been minimized. The Markov blankets demarcate the boundaries between different levels of the brain's generative model. Each blanket induces a conditional independence between the internal states of a given level and the external states of the levels above and below it. This allows each level to operate as a relatively autonomous unit, processing information at its own spatial-temporal scale and generating predictions and prediction errors that are tailored to its very specific level of abstraction.

Conscious experience is shaped by the deployment of covert attention, which refers to the selective enhancement or suppression of specific prediction errors based on their estimated precision or reliability. This process is mediated by top-down signals from higher levels of the hierarchy, which modulate the gain or sensitivity of lower-level sensory neurons. By selectively attending to specific areas of sensory data, the brain can effectively prioritize the information that is most relevant for updating its generative models, and this determines which sensory data are consciously perceived and which are filtered out or suppressed.
Now let's get to the temporal structure of consciousness, which must still be accounted for, and this is why we turn to Husserl. The inner screen theory posits that the hierarchical structure of the brain, with its Markov blankets, provides the foundation for the temporal dynamics of consciousness. According to Husserl's phenomenological account, consciousness exhibits a temporal thickness where the immediate presence (primal impressions) is contextualized by a retention of the just-past and an anticipation (protention) of the imminent future. This tripartite structure of time consciousness shapes the subjective flow of experience.
If we map Husserl's phenomenology to active inference:
1. Retention corresponds to the encoding of past experiences into the parameters of an agent's generative model. The matrices that you see in this diagram are represented by the little squares, and so the matrices A (the likelihood mapping) and B (the transition mapping) in the generative model represent sedimented knowledge and expectations based on previous encounters with the world. These matrices underwrite perceptual inferences and enable the agent to contextualize new sensory impressions against the backdrop of prior learning.
2. Primal impressions correspond to the new sensory data (the observations, the little o in that diagram) that the agent receives moment by moment. These impressions drive belief updating and learning. Primal impressions are always already embedded in a temporal horizon colored by retention and protention.
3. Protention refers to the agent's implicit anticipation of what will be experienced next based on the current state of the generative model. This anticipatory dimension is captured by the C outcomes and generally informed also by prior expectations and habits (D), which shape the agent's policy selection and action. Protention thus guides the agent's interaction with the world, enabling it to seek out expected observations and minimize surprise.
This is work that's been pioneered by the relationship between Bayesian inference and Husserl's phenomenology by Jakob Hohwy.
In the context of the Husserlian account of time consciousness, the Markov blankets at each level of the hierarchy can be thought of as encoding the retention, primal impression, and protention of the agent's experience at that particular level of abstraction. The information encoded on these screens represents the agent's sedimented knowledge and expectations, the current sensory input, as well as the anticipated future which comes from the higher levels. The temporal thickness of consciousness thus emerges from the exchange between these different levels of the hierarchy, each operating at its own spatial-temporal scale.
The lower levels of the hierarchy, which process more concrete and short-term representations, contribute to the primal impression of the immediate present. The higher levels, which encode more abstract and long-term representations, provide the context for this immediate experience by shaping the retention of past experiences and the protentions of future states. As information flows up and down the hierarchy through the process of writing and reading on the Markov blankets, the agent's conscious experience is continuously updated and refined. This selective attention to specific prediction errors, mediated by top-down signals from the higher levels, allows for the prioritization of relevant information, the suppression of irrelevant details, and shapes the contents of conscious experiences, underlying the dynamic flowing nature of time consciousness.
So we've explored consciousness, we've explored the nature of the temporal flow, now let's extend it to what we promised before to the social domain. The inner screen theory can account for the emergence of shared protentions among agents. In a multi-agent setting, each agent's generative model includes not only representations of the environment but also beliefs about other agents, which are part of the environment. These beliefs are encoded in the higher levels of the hierarchy where more abstract and long-term representations are processed. As agents interact and communicate, they exchange cues and signals that allow them to infer each other's mental states and intentions. This social inference process can be understood as a form of active inference, where agents update their beliefs about others based on the information they receive.
The Markov blankets at the higher levels of each agent's hierarchy encode the shared protentions that emerge from this social interaction. The alignment of protentions across agents is facilitated by the shared structure of their generative models, which is shaped by their common experiences and learning. The Markov blankets at the higher levels of the hierarchy encode the shared background knowledge and expectations that provide the basis for mutual understanding and coordination. The temporal structure of consciousness, with its inherent anticipatory dimension, enables agents to project themselves into the future and imagine possible outcomes of joint actions. Shared protentions thus underwrite the ability to engage in collaborative learning, collaborative planning, joint goal setting, and the pursuit of collective objectives. By aligning their protentions and coordinating their actions, agents can navigate the complexities of the social landscape and achieve outcomes that would be impossible for any individual alone.
In mathematical terms, the brilliant Toby Sinclair-Smith, which is one of the co-authors on one of our papers, explains that the alignment of protentions across agents can be modeled using the tools of category theory. By representing agents' generative models as polynomial functors and using sheaf-theoretic constructs to glue together these models, we can formalize the emergence of shared protentions and the dynamics of multi-agent coordination. The resulting consensus topos provides us with a mathematical image of the shared social world, capturing the ways in which individual and collective intentionality can be represented.
Now, in the context of social interactions, agents are not just driven to optimize their own beliefs but also to infer and predict the beliefs of others, and this is where the notions of shared protention come into play. In the social setting, shared protentions emerge as agents develop similar expectations and predictions about each other in their beliefs and behaviors. Agents equipped with generative models of their social environment engage in active sampling of information from their peers. This sampling process is biased by the agents' prior beliefs and expectations. This confirmation bias leads agents to preferentially attend to and seek out information that aligns with their pre-existing beliefs.
We propose the model of epistemic communities under active inference, which has an epistemic confirmation bias parameter gamma (we call it ECB for short), which captures this tendency. A higher value of gamma implies that agents believe that more reliable or informative observations come from like-minded peers. Consequently, agents with a high ECB or gamma are more likely to sample information from those whose beliefs they expect to align with their own, and you can see why it makes coordination easier. This selective sampling behavior reinforces the agent's prior beliefs and can lead to the formation of echo chambers where agents become increasingly isolated from divergent perspectives.
The role of social network structure shapes the dynamics of belief formation and polarization. The connectivity of the network, which we parameterize by the connection probability p in random graph models, determines the extent to which agents are exposed to diverse viewpoints. In sparsely connected networks (low p), agents have limited access to information from dissimilar others, which can facilitate the emergence of isolated belief clusters or echo chambers. Conversely, densely connected networks (high p) provide more opportunities for agents to encounter conflicting information, potentially countering the effect of confirmation bias. However, high levels of epistemic confirmation bias can still lead to polarization even in densely connected networks because the formation of epistemic communities is not solely determined by the network topology but also depends on the cognitive dispositions of the agents within the network.
The habit formation mechanism, captured by the learning rate parameter eta, further amplifies the dynamics of echo chambers. As agents repeatedly sample information from like-minded peers, they develop habitual patterns of interaction that become self-reinforcing over time. This habit learning process can make it increasingly difficult for agents to break out of their existing belief clusters even when exposed to novel information.
The key to facilitating the emergence of shared intelligence thus lies in our learnings about epistemic communities in the efficient exchange of information among agents but also in their belief dynamics. We can thus apply the principle of Markov blankets and the holographic screen theory from our inner screens theory, where Markov blankets define the probabilistic boundaries between an agent and its environment, mediating the flow of information. The holographic screen extends this concept and suggests that the boundaries between different levels of hierarchical generative models can be understood as screens encoding classical information.
By designing artificial intelligence agents with architectures that incorporate these principles, we can thus create networks in which information is efficiently processed and propagated, allowing for the emergence of collective dynamics. As agents interact and communicate within this framework, they can begin to develop shared narratives or goals through their shared protention. This process is driven by the minimization of variational free energy, which encourages agents to seek out information that aligns with their existing beliefs but also update their beliefs based on new evidence. Active inference allows for this balancing act. Over time, this leads to the formation of stable, mutually reinforcing belief structures that span the entire network. These shared narratives provide a common ground for collaboration and collective problem-solving, allowing agents to potentially seek out alternative evidence that they can then integrate into the group as agents work together to minimize uncertainty and maximize their evidence for their collective generative model.
And there we get to the last part of this talk, which is this leads us to the potential for the emergence of shared narratives and goals, which marks the transition from individual sentient intelligence to sympathetic intelligence. At this stage, agents are capable of recognizing and understanding beliefs, intentions, and dispositions of other agents in the network. This form of perspective-taking enables more sophisticated forms of cooperation and coordination as agents can anticipate and respond to the actions of others in ways that benefit the collective.
As sympathetic intelligence continues to develop, the network may give rise to shared intelligence, where a collective intelligence emerges from the interactions of multiple agents. This shared intelligence is characterized by the ability to integrate and synthesize the knowledge and capabilities of individual agents, allowing the network as a whole to solve problems and adapt to challenges that would be beyond the scope of any single agent, which is also limited to the evidence it had access to. This is an extension of the inner screen theory, where we can now identify new Markov blankets extended beyond single agents, which constitute holographic screens in themselves.
The principles of active inference give us here a common computational framework that can be applied to both artificial and biological intelligences. It opens up the possibilities for creating hybrid ecosystems of intelligence in which human and artificial intelligence can interact and collaborate seamlessly. If we ground these ecosystems in the same fundamental principles of information processing and belief updating, we can facilitate the emergence of shared narratives and goals that span the divide between natural and artificial intelligence.
So in conclusion, if we recap a little bit, the inner screen theory of consciousness grounded in active inference hopes to provide us with a unifying perspective on the emergence and dynamics of subjective experience but also gives us insights for the development of artificial intelligence. When we understand how intelligence arises from the collective dynamics of interacting agents, we can design artificial ecosystems where information is computed more effectively and problem-solving capabilities exceed those of the individual parts. It can guide us in the creation of efficient information processing architectures in artificial agent networks while the formation of shared narratives and goals can also foster collaboration and collective intelligence. As we move towards this future where natural and artificial intelligence agents have to coexist and collaborate, understanding the fundamental principles underlying both biological and artificial cognition will be essential for creating harmonious and productive ecosystems of intelligence. And there we go. Thank you very much for your presentation.
Human: So I think it would be nice if we moved on to questions in case someone wants to post one.
Assistant: So I think yeah, you can go on please. The person who raised the hand online.
Human: Me? Yeah, I actually wanted to upload and then I clicked the raise hand button, but I also have a question so it's good to jump straight to it. Thank you so much for the presentation. It was, I wanted to say it was kind of in terms of seeking novelty or being satisfied by evidence, I was like having questions that you answered with the next few slides every time so it was very satisfying but at the same time I was left without questions as I went on. One of the things that I think a lot about of course is this, so I'm, to introduce myself I'm a PhD researcher in AI and I use a lot of active inference material for my work. One of the things that I think a lot about is of course this kind of the problem between the division of where the kind of if we want to think about agency control etc. larger scale dynamics, where is the division between the agent and the larger scale systems, right? And in the context of creating artificial intelligence I really love your ending remarks for thinking towards new paradigms with this, but then I wonder in the context of things that are just the purely contingent stuff we have to deal with in everyday life, we already have the experience of a lot of things that kind of challenge us with these scenarios such as for example people with that are considered mentally pathological, people that sort of fall out of the general hegemonic framework of how you should function as a system that is and strives towards. So I wonder if you have, and it's a very egoistic question because I'm presenting on this subject next week so I would love your feedback. I wonder if you have any...
Assistant: Absolutely. So that's a really great question. That's a question that a lot of people in the field such as Mark Miller, Inês Hipólito, and myself are working on. They use the concept of a psychotic Markov blanket, and effectively, and I know that Mark Miller specifically uses the term "bad bootstrap". Think of agents as doing their best, right? That doesn't mean they have found the optimal solution, it means that given the evidence they have acquired in their life, they are doing their best. And there's also super stimuli that sometimes play with our brains in the sense that we've evolved over hundreds of thousands of years to understand certain signals as meaning certain things. So like for instance, sugar in candy is a super stimulus. We've evolved to know that sugar is good, but too much sugar is bad. We've just not evolved yet to fully push that away. So our brain identifies this as "wow, this is really good".
And so oftentimes a bad bootstrap will happen when you sort of reinforce certain pathways, certain beliefs about the world that maybe disagree with the rest of the world. But what will happen is you'll start to close off the evidence that is bad because that stimulus is so strong that you can't get away from it. I know Mark Miller is studying this in the context of, for instance, drug abuse. Drug abuse is a pattern that is difficult because it doesn't just make you want drugs, it also cuts you off from the rest of the world such that even if you wanted to, you have lost all access to other policies. You can lose your job, you lose your social network, you lose money. So now you're stuck within this very constrained set of policies which only enable you to sample a very small subset of reality and therefore continues to reinforce this pattern. So that's a super interesting question. I'm looking forward to seeing your work and I strongly encourage you to reach out to Mark Miller and Inês Hipólito because they will have thoughts about this as well.
Human: Thank you so much.
Assistant: Thank you very much for your question and the answer was really good as well. Regarding something that came into my mind about psychotic states and active inference, I know that Isomura 2022 looks into that a bit at the end of the paper. So yeah, if you want to have a look. Okay, we have time for more questions, so please raise your hand if you have one. Okay, I do have some question. Oh well, we have a David Hiland. Please go ahead.
Human: Hi David.
Assistant: Hey David.
Human: Hey, thanks M for the great talk. It was really interesting and yeah, I just want to ask you a question about this idea of coordination among different agents to produce collective behavior. Because one thing I was wondering about is how does the notion of competition and sort of specialization of function fit into this framework? You know, because you see this a lot in things like the brain where neurons will compete for activation or different brain regions will compete for processing resources, or you know, companies will compete in an economy to promote overall economic growth. So I'm wondering how that fits into your thinking.
Assistant: That's a great question. So that's part of also what we're working on. So we've got a paper that we wrote on resilience. It also builds on Noor Eissa, I think is her name, and Karl Friston's work on degeneracy and redundancy, and we built, we wrote this paper on resilience which basically casts active inference as a function of inertia, elasticity, and plasticity, but basically explains that you develop your system as a function of your ability to derive degeneracy. And so what's degeneracy in this context? It's a degree of redundancy that has functional variability, right? So if you have all the parts of your system that can do the same thing, you accrue a lot of complexity and not a lot of benefit, right? There's diminishing returns. However, if some parts of your system can replace a part that may be lost to error but in the case where it's not needed can also do something else, so think of your hands. I know that's the example that Noor used for playing the piano. I don't need both my hands to play the same thing. I can use one to do this and use the other to do that, and they're technically both similar functions, right? But they now have extended the realm of what I can do.
Human: Thank you very much for your answer. We still have time for more questions if not we'll continue this. We'll put another question okay. I'm curious about how I know please uh David go on you go first.
Human: Yeah so I guess another question related to these interactions between different active inference agents is that I really like the framing of viewing these interactions as communications between different agents sharing of beliefs and so forth but I'm also wondering about agents acting on their shared environment. How does this kind of fit into your picture do you think that all actions on a shared environment constitute a form of communication or is this something that we should think about separately? What are your thoughts on that?
Assistant: I was so ready for this I pulled up all of the PDFs but I don't want to share different windows I'm afraid so like I'm going to pull up this paper and obviously please this paper work is building on the works of many other authors so please go look at the references especially the references from Axel Constant, Maxwell Ramstead, Samuel Veissière and many others. But effectively yes, so agents do not simply talk to each other and not affect the world. What we tried to show is that the world also effectively acts as a Markov blanket and therefore embodies a joint distribution and therefore a form of generative model and so when you interact with the world you write onto the world and the world reads from you and vice versa the world writes to you and you read from the world. So this is why most of our systems or at least the systems that we care about are chaotic systems of second order because everything we do changes the system. Just being able to predict the world changes the system as well. This being said there are patterns and not constants but things that we can latch on to. That's why technologically we came up with culture. Culture is a marvelous human invention not just human but let's say a cognitive invention that gives us some degree of understanding of where we stand relative to our environment in a given context. It tells us information it pulls these priors into us such that we can effectively act in the world without having to learn everything from scratch every time. So we've sort of showed in this paper and again lots of work by Axel Constant please go check it out. This is the figure I showed in the talk where basically you can consider that the world's prior also changes. We use this example of the grass when you choose as an agent to take the sidewalk walk you can in fact use the cues that are in the world to walk on the sidewalk but then one of us will decide nope I'm walking on the grass and this will sort of start something like maybe somebody else saw them and like oh okay I guess it's acceptable to walk on the grass and eventually as more people walk on the grass a grass path gets generated by the environment grass stops growing there it's like well all right fine I won't grow there I'll just make the path for you. So the environment and the individual shape each other mutually and we can see these as sort of attractors and that's what culture is. Culture is the registering the demarcation of these attractors that we know of. The problem with attractors is that they're not necessarily stable either and as I interpreted grass as a potential sidewalk you can see that these concepts are porous they have the possibility to pull from or change a little bit and thus it moves and so your attractors move as well and your culture therefore will move with the attractors and so will you and so some people actively will try to counter discourse the culture and this is what Althusser kind of talks about and Judith Butler also talks about this we are never really and I think Foucault forget his name we are never really outside of culture we are always stuck within these ideological paths and we can try to move past them but there's always sort of an echo of these paths because even counter culture is a reaction to something that was already there. So that's a lot of words to say it's a good question we don't really have the full answer yet because otherwise we would have solved emergence chaotic systems but there is a pathway there that we can sort of maybe start to computationally make these questions more tractable.
Human: That's really fascinating, conceiving taking actions as an interaction or communication with the world and yeah I'm wondering I guess as a follow up to that whether you can kind of think of the world as almost like an outer screen where that's kind of you know the stage that everyone interacts with or yeah I don't know what are your thoughts on that?
Assistant: Can you clarify your question a little bit whether society is a screen?
Human: Well yeah I mean I guess like you know if you think about let's say researchers collaborating on a whiteboard you kind of using your external environment as a shared screen for coordination um in maybe in a similar manner to how you talk about the inner screen or you know in relation to things like the global neuronal workspace.
Assistant: No exactly exactly exactly. So we've developed these tools like writing or semantic markers in the world that allow us to derive social scripts right? So for instance I know to sort of look at the camera because if I looked over there that wouldn't make a whole lot of sense. I could technically but my affordance is directed towards the camera and similarly we are writing with letters we've developed these tools that where I can now share information that will stay a little longer in my brain and therefore allow me to push the computation ever so closer but also because there's a pragmatic link between us to these physical elements there's something I can sort of collapse when I understand what you're saying because we're both speaking English but I don't know if you really mean what I think you mean it's just that I can sort of derive that we share so much in common based on the things I can read about you based on the facts that we share the same kinds of patterns you're in this conference I'm in this conference I assume we have shared knowledge and therefore we can talk and from there we are both capturing some part of the computation and we can push it further by this very conversation we've pushed it a little bit further then maybe I would have gone on my own so yes absolutely this is why I was trying to explain in the talk that the boundary between the self and the world is actually not that obvious right it's the whole extended cognition paradigm where your cognition doesn't just happen in your brain it happens all throughout your body it happens all throughout the things which you have access to directly that enable this computation.
Human: Yeah Jager did you you raise your hand right go go on please.
Human: Yeah if I'm not going ahead of someone who wanted to ask something no okay now I thank my question is is related I was just reminded I've been speculating about this after seeing the talk I don't know if you saw the on the active inference YouTube channel the Jamie Joyce Library Society presentation that they did really interesting like uh one of the projects that they're doing is kind of like opinion mapping and looking at how you can yeah well instead of looking at whether people agree with something or not or whether like you can actually just map out the structure of debates and look at the entirety of a sort of discourse landscape uh just by looking at you know where people go how opinions sort of diverge how you take one path or another depending on the information that you receive and I was thinking about oh it's really interesting to imagine something like that but then turbo charged uh like a kind of a mapping tool that you could use to measure the degree of sort of future domination that certain systems have or not you know so that to look at how much of I don't know a particular type of industry or a particular type of cognitive process like like yeah any kind of ideological positioning how much that is affecting sort of the futurity of the entirety of the of the social or biological system whatever it is that we're interested in in mapping and I wonder yeah if you have any thoughts on this kind of idea of so we have the temporality of looking at pretensions we have the sort of temporal light cone from Lean idea and the screens now the holographic screens do you see any do you have any visions for how we might be able to kind of think about formalizing the looking at how yeah how much of the future something is capturing in a sense how much of the future it's yeah.
Assistant: I mean yeah effectively this was the purpose of the work on scripts theory and this entire endeavor is intended to lead this if you have a video or some papers you could send me relative to this talk it seems extremely relevant but effectively yeah so what we can show with this model is the kind of beliefs that agents are likely to be swayed by relative to their current beliefs and how the topology of their social network globally could lead them to sway one way or another. So if they have certain kinds of beliefs and they are you can identify that they sometimes sway a little bit you could you can sort of identify their ECB right their epistemic confirmation bias and you can sort of see okay well they're highly unlikely to look at any other kind of evidence so what's the other alternative well the alternative is looking at their community their beliefs and the ways in which you can sort of pull on the web of their semantic beliefs because all beliefs are sort of connected in a web some beliefs are incompatible and they're sort of kept slightly separate like you can have this in it beliefs but as soon as you present that to the person they'll recoil really fast like they're they're not going to try and engage with that if they engage with that you can sort of make them change their mind but let's assume they're not let's assume they have dissonant beliefs but that to some degree some of the webs of their beliefs are connected you can sort of pull on some parts to bring them to something else so this is why a lot of ideologies are extremely correlated right so intolerant ideologies tend to correlate with other intolerant ideologies because the premise is relatively similar so when you pull on one you tend to pull them with the rest and it's the same thing for the other side right we we can sort of imagine that an ideology that has empathy for a class struggle for instance is potentially likely to also have empathy for other kinds of struggles and therefore integrate that into their model so there are ways to pull on these threads to move people around what you can do is try to see given known discourses and obviously this would be extremely computationally expensive so I'm not saying this is how we're going to do it but we can assume how given types of discourses are likely to sort of move their attractors based on that you can also see how the different networks are likely to evolve as a function of either being pulled towards or away from them and then you can sort of see whether these are likely to become the dominant ideology or not so it is possible it would let this kind of approach would lend itself nicely to it but the key problem is still the computational tractability of these things I'm sure that if some big tech company decided that they wanted to put all its resources like something like I don't know Pegasus or something wanted to do that they could they have the information they have the computational capabilities and the funds but I like to think we can use it for good I like to think we can use it to promote you know peace in the world this sort of thing obviously any technology can always be used for other for its counterpart as well.
Human: No that's fascinating thank you yeah and also yeah you have the modeling the model problem all over again to with the degree of granularity right that you would be wanting there but yeah thank you.
Human: More questions okay. I think we had a really nice discussion and I hope everyone thinks the same. We would like to invite anyone of our listeners to at some point if they well we are open for discussions for presentations so if anyone of the attendees think that would like to present their work more or less on this area or if you know of someone who you think that would be interesting to listen into this group please feel forward to to send an email to us we will. Ralph is going to post his email or my email in the chat group now so so everyone has it and yeah I think that's it. Let's uh well your email you can just put your email and ah well yes use this one yeah that's good one second all good so this is the email from Humane dialogues and thank you very much for Mah hold for attending and giving this amazing presentation and keeping up this great great discussion and so well for all the attendees thanks a lot for coming and we hope to see you in the next event thank you thank you very much bye everyone.