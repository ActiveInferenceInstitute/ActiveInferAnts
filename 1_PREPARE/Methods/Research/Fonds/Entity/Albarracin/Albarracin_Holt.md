Beyond LLMs: Mahault Albarracin on How Active Inference Gives AI a Sense of Self in the World https://www.youtube.com/watch?v=MmsgmHONVi0

Chapters: 00:00 - Introduction: The Need for a New AI Paradigm 01:58 - Understanding Active Inference and AI's Sense of Self 06:39 – The Obligatory Commentary on the OpenAI Drama 10:19 - The Role of Active Inference in AI's Evolution 17:44 - Insights from Working with Karl Friston 21:43 - Interdisciplinary Approach to AI Research 31:56 - AI and the Future of Work and Society 43:35 - AI Alignment, Empathy, and Consciousness 54:51 - Tailoring AI's Empathy and Sensitivity 58:51 – AI Embodiment 01:07:59 - Navigating AI Governance and Inclusion

A new architecture for AI much more powerful than LLMs is about to come into the world. It's called active inference. There's been a lot of discussion in the AI Community about the need for a new architecture and AI paradigm shift to continue evolving the technology. Just this morning heavy discussion was sparked online about a paper that was published in Sage journals in October that centered around what children can do that large language and language in Vision models cannot. This paper argues that the best way to think about these systems is as powerful new cultural Technologies but that they lack the ability to innovate. LLMs do not have the ability to actually perceive the world and act on it in new ways. It's the difference between imitation and Innovation. As this paper points out, nothing in their training or objective functions is designed to fulfill the act of Truth seeking such as perception causal inference or Theory formation. So why is this? I encourage you to tune in to this episode of my podcast conversation with Mahault Albarracin, director of product for research and Innovation at Verses AI as we explore a critical piece that is missing in current AI models: a sense of self. So what does this mean and how do active inference intelligent agents achieve this sense of themselves in the world and what does this mean for the evolution of cognitive Technologies? As we Marvel at the prowess of pre-trained Transformer models like chat GPT Claude and others it's time to investigate a bit further and seek answers beyond the hype and Fascination lies a catalyst for Change and remarkable Innovation fundamentally redefining what AI means this is active inference AI.

Understanding Active Inference and AI's Sense of Self When we talk about LLMs what we're really talking about is content to content prediction. What you're getting is a piece of content that doesn't intrinsically have any meaning right it's just a piece of content that generally is placed in a hierarchy of other contents and what you're going to do is predict the most likely x amount of word given the selected words given an attention Paradigm and that's super useful. It's very efficient for tasks that do not require a lot of cognition they require hardcoded knowledge in a sense. We generally use active inference to talk about living organisms self-organizing organisms so that includes humans but eventually that could include AI systems because the key part of living is mostly self-organization. Basically what we do is that we perceive information and then we act in the world by constantly making predictions about our environment based on the outcomes of our actions and so then we update these predictions based on the sensory input that returns from our actions as well. It would involve a continuous cycle of hypothesis testing which is not something LLMs currently do. LLMs when they are giving you an output have no sense of themselves in the world they do not understand what they're saying or what the outcome of what they're saying is going to be and where they are relative to what they were asked they don't have a sense of self. So then they don't really update their internal model based on the prediction errors most of these systems are static. So now let's say you gave an LLM the capacity to continuously update. All right so that's a step forward right you would have the capacity to enable the system to adapt a little bit more but it would kind of adapt blindly because again active inference is related to the boundary of the system and the relationship between the internal states of the system and the external world. This joint distribution is Central it's key to have a sense of this joint distribution you need to have a sense of perspective you need to have a sense of self relative to the world and to what enables you to fulfill your goals and once you have all of that then you have a sense of being a sense that gives you a position into the world with that position you can now predict the outcomes of your actions and understand where you are relative to the request to the input to what you are trying to do I think that's what's missing currently.

The Obligatory Commentary on the OpenAI Drama Hi and welcome back to another episode of the spatial web AI podcast today I have a wonderful guest with me Mahault Albarracin. Mahault is the director of product for research and Innovations in active inference. Mahault thank you so much for coming on our show today it's such a pleasure to have you here. Why don't you introduce yourself to our audience and kind of give them an idea of what kind of work you're involved in? Sure so thanks for having me. I am in fact director of product for research Innovations but I'm also doing a PhD in cognitive Computing I have a masters in social sciences I'm affiliated with the institute for health and society and for the institute for feminist research at UQAM which is the university in Montreal. The work I do generally touches on social systems simulations and how you can predict and understand belief changes given certain kinds of semantic or meaning fields that are shared across different groups and how those can evolve over time. Very nice very nice well I'm so excited to dive into this conversation with you today. Before we get started down the path of active inference though you know what a bombshell we just had this week weekend with open AI and Sam Altman and all of that. Do you have any thoughts you'd like to share since I know it's top of mind for everybody right now? Right so I guess everybody's going to be a little worried about the future of open AI and about the future of their tools that were being widely used by everybody. I think the key question is one what is going to happen to the open source model and what is going to happen to the nonprofit aspect of open AI at least as it was stated before. But beyond this this seems more like a political battle like it seems to be about certain kinds of decisions that were internally not fully aligned. I think we as people who work in the field have to stay focused and stay focused specifically on what do we want for the world what we want for the future how are we sourcing those directions instead of focusing on one entrepreneur or one of the tools. There are many people giving thoughts to these ideas and I think we should mostly be worried about a large Monopoly that may not be giving us the tools to potentially influence where these kinds of questions are going. Microsoft is a wonderful company it does tremendous work but it is one big company we really need to make sure that there is a plethora of approaches to this very same problem such that we don't necessarily fall into a dip and then consider that this is the only approach we should be using. Yeah totally agree with that. You know one of the thoughts that I had you know just a couple days before all this started you know Sam became very vocal about LLMs can't lead to AGI so my I'm just so curious what their research will lean to now so yeah those are things that I've been thinking about. So that's actually a really interesting question I think we tend to think of AI as the tool but for now AI is the expertise what matters is who the people you have understand of the field and how they manage to innovate and build upon past knowledge so it doesn't really matter whether they use GPT or not what matters is what are the people that were brought over there understanding of what's required to move forward and if if you if you are correct about what Sam Altman is saying that LLMs will not bring us to AGI I think I think we all kind of knew that it's not sufficient to have a very very proficient language model because while language is an intrinsic part of our cognition it is not sufficient because for instance if you put the words in a different order if you scrambled the probabilities in the models you wouldn't have a different cognition right you would just have nonsense. What you need is a model that's capable of understanding a structure of the world there's a notion of understanding here that cannot simply be latent in the static knowledge that captured by probabilities of language. Great Point.

The Role of Active Inference in AI's Evolution So that brings me then to you know the most exciting aspect of me having you on the show is let's dive into a little bit the difference between the LLMs and the active inference and what is your opinion on why you know active inference is likely to play a hugely pivotal role in the future of AI? Okay so when we talk about LLMs what we're really talking about is content to content prediction what you're getting is a piece of content that doesn't intrinsically have any meaning right it's just a piece of content that generally is placed in a hierarchy of other contents and what you're going to do is predict the most likely x amount of word given the selected words given an attention Paradigm and that's super useful it's very efficient for tasks that do not require a lot of cognition they require hardcoded knowledge in a sense. So when we think about intelligence we can think of crystallized intelligence so just knowledge if I ask you a trivia question can you give me back the answer to that trivia question it's different say from an IQ test where it's not all about knowledge it's mostly about your capacity to derive the structure of a problem which is something an LLM cannot do you need to be able to understand how different pieces interact with each other and understand the deeper foundations that might give to a prediction and potentially even a volatile environment prediction so not I've seen this before and therefore I can reproduce it now it's more I have never seen this before but it follows certain kinds of patterns that I might have encountered or if I combine these two things together I'm going to infer given their properties a new type of property and and that's how I'm going to move towards a new prediction so that's roughly why active inference or at least model-based approaches lead us to potentially closer to what we consider to be AGI. Interesting yeah so what you're touching on there then in that explanation is how our brains work with the free energy principle predicting rationalizing the cause of what we're interpreting through our sensory input and and then measuring it against what we know to be true in the this internal model we've already established and then correcting from there you know I mean that's the basic way that I try to explain this to people but I'm sure you can do a much better job than me and I would love to hear how you would explain the free energy principle to somebody here. Sure so I'm going to explain the difference between LLMs and and active inference and that's going to allow us to dig into the free energy principle. So LLMs are basically designed for generating and manipulating just human language right they are trained on a vast amount of data to predict the next word in a sentence essentially you have supervised learning over a large Corpus of text so they learn just statistical patterns just statistical patterns and which is just essentially adjusting weights in the neural network to optimize to minimize the difference between the predicted output and the actual output. So this sounds very similar to what we think we're doing with active inference right we're just trying to minimize difference between an input or a predicted output and the actual output. In active inference though there's a little bit more going on under the hood so basically we have we generally use active inference to talk about living organisms self-organizing organisms so that includes humans but eventually that could include AI systems because the key part of living is mostly self-organization it doesn't require the system to breathe it does require the system to try to maintain its current state space let's say the way that it's distributed over SpaceTime and so basically what we do is that we perceive information and then we act in the world by constantly making predictions about our environment based on the outcomes of our actions and so then we update these predictions based on the sensory input that returns from our actions as well. So like difference isn't just a specific model it's a theory about how intelligence systems could operate you could have an LLM that uses active inference but it would involve a continuous cycle of hypothesis testing which is not something LLMs currently do. LLMs when they are giving you an output have no sense of themselves in the world they do not understand what they're saying or what the outcome of what they're saying is going to be and where they are relative to what they were asked. They don't have a sense of self so then they don't really update their internal model based on the prediction errors most of these systems are static. So now let's say you gave an LLM the capacity to continuously update all right so that's a step forward right you would have the capacity to enable the system to adapt a little bit more but it would kind of adapt blindly because again active inference is related to the boundary of the system and the relationship between the internal states of the system and the external world this joint distribution is Central it's key to have a sense of this joint distribution you need to have a sense of perspective you need to have a sense of self relative to the world and to what enables you to fulfill your goals and once you have all of that then you have a sense of bias a sense that gives you a position into the world with that position you can now predict the outcomes of your actions and understand where you are relative to the request to the input to what you are trying to do I think that's what's missing currently. It doesn't have to be done through the free energy principle so the free energy principle is it's quite simple in itself it's simply saying you're trying to minimize a quantity known as free energy which in terms of information is just surprise you are trying to put yourself in a place where you are likely to continue existing and therefore self-evidence in this way you want to make sure you're constantly in a place where nothing is going to come and disrupt your boundary you can only do this by continuously learning and by you know exploiting your environment so you're trading off constantly between exploring and exploiting. Interesting yeah I love this I also love the point of it requires a sense of self because that's something that I hadn't really put together and that that that kind of brings it all together in the sense of why why these other systems aren't capable of that wow okay so another question then because I know you know I know you work closely with Karl Friston uh what is it like to work with Karl I know there's going to be people out there that are wondering just like I am.

Chapters:
00:00 - Introduction: The Need for a New AI Paradigm
01:58 - Understanding Active Inference and AI's Sense of Self
06:39 – The Obligatory Commentary on the OpenAI Drama
10:19 - The Role of Active Inference in AI's Evolution
17:44 - Insights from Working with Karl Friston
21:43 - Interdisciplinary Approach to AI Research
31:56 - AI and the Future of Work and Society
43:35 - AI Alignment, Empathy, and Consciousness
54:51 - Tailoring AI's Empathy and Sensitivity
58:51 – AI Embodiment
01:07:59 - Navigating AI Governance and Inclusion

A new architecture for AI much more powerful than LLMs is about to come into the world. It's called active inference. There's been a lot of discussion in the AI Community about the need for a new architecture and AI paradigm shift to continue evolving the technology. Just this morning heavy discussion was sparked online about a paper that was published in Sage journals in October that centered around what children can do that large language and language in Vision models cannot. This paper argues that the best way to think about these systems is as powerful new cultural Technologies but that they lack the ability to innovate. LLMs do not have the ability to actually perceive the world and act on it in new ways. It's the difference between imitation and Innovation. As this paper points out, nothing in their training or objective functions is designed to fulfill the act of Truth seeking such as perception causal inference or Theory formation. So why is this? I encourage you to tune in to this episode of my podcast conversation with Mahault Albarracin, director of product for research and Innovation at versus AI as we explore a critical piece that is missing in current AI models: a sense of self. So what does this mean and how do active inference intelligent agents achieve this sense of themselves in the world and what does this mean for the evolution of cognitive Technologies? As we Marvel at the prowess of pre-trained Transformer models like chat GPT Claude and others it's time to investigate a bit further and seek answers beyond the hype and Fascination lies a catalyst for Change and remarkable Innovation fundamentally redefining what AI means this is active inference AI.

Understanding Active Inference and AI's Sense of Self
When we talk about LLMs what we're really talking about is content to content prediction. What you're getting is a piece of content that doesn't intrinsically have any meaning right it's just a piece of content that generally is placed in a hierarchy of other contents and what you're going to do is predict the most likely x amount of word given the selected words given an attention Paradigm and that's super useful. It's very efficient for tasks that do not require a lot of cognition they require hardcoded knowledge in a sense. We generally use active inference to talk about living organisms self-organizing organisms so that includes humans but eventually that could include AI systems because the key part of living is mostly self-organization. Basically what we do is that we perceive information and then we act in the world by constantly making predictions about our environment based on the outcomes of our actions and so then we update these predictions based on the sensory input that returns from our actions as well. It would involve a continuous cycle of hypothesis testing which is not something LLMs currently do. LLMs when they are giving you an output have no sense of themselves in the world they do not understand what they're saying or what the outcome of what they're saying is going to be and where they are relative to what they were asked they don't have a sense of self. So then they don't really update their internal model based on the prediction errors most of these systems are static. So now let's say you gave an LLM the capacity to continuously update. All right so that's a step forward right you would have the capacity to enable the system to adapt a little bit more but it would kind of adapt blindly because again active inference is related to the boundary of the system and the relationship between the internal states of the system and the external world. This joint distribution is Central it's key to have a sense of this joint distribution you need to have a sense of perspective you need to have a sense of self relative to the world and to what enables you to fulfill your goals and once you have all of that then you have a sense of being a sense that gives you a position into the world with that position you can now predict the outcomes of your actions and understand where you are relative to the request to the input to what you are trying to do I think that's what's missing currently.

The Obligatory Commentary on the OpenAI Drama
Hi and welcome back to another episode of the spatial web AI podcast today I have a wonderful guest with me Mahault Albarracin. Mahault is the director of product for research and Innovations in active inference. Mahault thank you so much for coming on our show today it's such a pleasure to have you here. Why don't you introduce yourself to our audience and kind of give them an idea of what kind of work you're involved in? Sure so thanks for having me. I am in fact director of product for research Innovations but I'm also doing a PhD in cognitive Computing I have a masters in social sciences I'm affiliated with the institute for health and society and for the institute for feminist research at UQAM which is the university in Montreal. The work I do generally touches on social systems simulations and how you can predict and understand belief changes given certain kinds of semantic or meaning fields that are shared across different groups and how those can evolve over time. Very nice very nice well I'm so excited to dive into this conversation with you today. Before we get started down the path of active inference though you know what a bombshell we just had this week weekend with open AI and Sam Altman and all of that. Do you have any thoughts you'd like to share since I know it's top of mind for everybody right now? Right so I guess everybody's going to be a little worried about the future of open AI and about the future of their tools that were being widely used by everybody. I think the key question is one what is going to happen to the open source model and what is going to happen to the nonprofit aspect of open AI at least as it was stated before. But beyond this this seems more like a political battle like it seems to be about certain kinds of decisions that were internally not fully aligned. I think we as people who work in the field have to stay focused and stay focused specifically on what do we want for the world what we want for the future how are we sourcing those directions instead of focusing on one entrepreneur or one of the tools. There are many people giving thoughts to these ideas and I think we should mostly be worried about a large Monopoly that may not be giving us the tools to potentially influence where these kinds of questions are going. Microsoft is a wonderful company it does tremendous work but it is one big company we really need to make sure that there is a plethora of approaches to this very same problem such that we don't necessarily fall into a dip and then consider that this is the only approach we should be using. Yeah totally agree with that. You know one of the thoughts that I had you know just a couple days before all this started you know Sam became very vocal about LLMs can't lead to AGI so my I'm just so curious what their research will lean to now so yeah those are things that I've been thinking about. So that's actually a really interesting question I think we tend to think of AI as the tool but for now AI is the expertise what matters is who the people you have understand of the field and how they manage to innovate and build upon past knowledge so it doesn't really matter whether they use GPT or not what matters is what are the people that were brought over there understanding of what's required to move forward and if if you if you are correct about what Sam Altman is saying that LLMs will not bring us to AGI I think I think we all kind of knew that it's not sufficient to have a very very proficient language model because while language is an intrinsic part of our cognition it is not sufficient because for instance if you put the words in a different order if you scrambled the probabilities in the models you wouldn't have a different cognition right you would just have nonsense. What you need is a model that's capable of understanding a structure of the world there's a notion of understanding here that cannot simply be latent in the static knowledge that captured by probabilities of language. Great Point.

The Role of Active Inference in AI's Evolution
So that brings me then to you know the most exciting aspect of me having you on the show is let's dive into a little bit the difference between the LLMs and the active inference and what is your opinion on why you know active inference is likely to play a hugely pivotal role in the future of AI? Okay so when we talk about LLMs what we're really talking about is content to content prediction what you're getting is a piece of content that doesn't intrinsically have any meaning right it's just a piece of content that generally is placed in a hierarchy of other contents and what you're going to do is predict the most likely x amount of word given the selected words given an attention Paradigm and that's super useful it's very efficient for tasks that do not require a lot of cognition they require hardcoded knowledge in a sense. So when we think about intelligence we can think of crystallized intelligence so just knowledge if I ask you a trivia question can you give me back the answer to that trivia question it's different say from an IQ test where it's not all about knowledge it's mostly about your capacity to derive the structure of a problem which is something an LLM cannot do you need to be able to understand how different pieces interact with each other and understand the deeper foundations that might give to a prediction and potentially even a volatile environment prediction so not I've seen this before and therefore I can reproduce it now it's more I have never seen this before but it follows certain kinds of patterns that I might have encountered or if I combine these two things together I'm going to infer given their properties a new type of property and and that's how I'm going to move towards a new prediction so that's roughly why active inference or at least model-based approaches lead us to potentially closer to what we consider to be AGI. Interesting yeah so what you're touching on there then in that explanation is how our brains work with the free energy principle predicting rationalizing the cause of what we're interpreting through our sensory input and and then measuring it against what we know to be true in the this internal model we've already established and then correcting from there you know I mean that's the basic way that I try to explain this to people but I'm sure you can do a much better job than me and I would love to hear how you would explain the free energy principle to somebody here. Sure so I'm going to explain the difference between LLMs and and active inference and that's going to allow us to dig into the free energy principle. So LLMs are basically designed for generating and manipulating just human language right they are trained on a vast amount of data to predict the next word in a sentence essentially you have supervised learning over a large Corpus of text so they learn just statistical patterns just statistical patterns and which is just essentially adjusting weights in the neural network to optimize to minimize the difference between the predicted output and the actual output. So this sounds very similar to what we think we're doing with active inference right we're just trying to minimize difference between an input or a predicted output and the actual output. The in active inference though there's a little bit more going on under the hood so basically we have we generally use active inference to talk about living organisms self-organizing organisms so that includes humans but eventually that could include AI systems because the key part of living is mostly self-organization it doesn't require the system to breathe it does require the system to try to maintain its current state space let's say the way that it's distributed over SpaceTime and so basically what we do is that we perceive information and then we act in the world by constantly making predictions about our environment based on the outcomes of our actions and so then we update these predictions based on the sensory input that returns from our actions as well. So like difference isn't just a specific model it's a theory about how intelligence systems could operate you could have an LLM that uses active inference but it would involve a continuous cycle of hypothesis testing which is not something LLMs currently do. LLMs when they are giving you an output have no sense of themselves in the world they do not understand what they're saying or what the outcome of what they're saying is going to be and where they are relative to what they were asked. They don't have a sense of self so then they don't really update their internal model based on the prediction errors most of these systems are static. So now let's say you gave an LLM the capacity to continuously update all right so that's a step forward right you would have the capacity to enable the system to adapt a little bit more but it would kind of adapt blindly because again active inference is related to the boundary of the system and the relationship between the internal states of the system and the external world this joint distribution is Central it's key to have a sense of this joint distribution you need to have a sense of perspective you need to have a sense of self relative to the world and to what enables you to fulfill your goals and once you have all of that then you have a sense of bias a sense that gives you a position into the world with that position you can now predict the outcomes of your actions and understand where you are relative to the request to the input to what you are trying to do I think that's what's missing currently. It doesn't have to be done through the free energy principle so the free energy principle is it's quite simple in itself it's simply saying you're trying to minimize a quantity known as free energy which in terms of information is just surprise you are trying to put yourself in a place where you are likely to continue existing and therefore self-evidence in this way you want to make sure you're constantly in a place where nothing is going to come and disrupt your boundary you can only do this by continuously learning and by you know exploiting your environment so you're trading off constantly between exploring and exploiting. Interesting yeah I love this I also love the point of it requires a sense of self because that's something that I hadn't really put together and that that that kind of brings it all together in the sense of why why these other systems aren't capable of that wow okay so another question then because I know you know I know you work closely with Karl Friston uh what is it like to work with Karl I know there's going to be people out there that are wondering just like I am.

Insights from Working with Karl Friston
So he is one of my PhD advisers. He's a very nice man very British and uh
to people but I'm sure you can do a much better job than me and I would love to
hear how you would explain the free energy principle to somebody here sure so I'm going to explain the difference
between LMS and and and active inference and that's going to allow us to dig into the free energy principle so sure um LM
is basically designed to is for for generating and manipulating just human language right they are trained on a
vast amount of data uh to predict the next word in a sentence um essentially
you have supervised learning over a large Corpus of text uh so they learn just statistical patterns just
statistical patterns um and which is just essentially adjusting weights in the neural network to op to minimize the
difference between the predicted output and the actual output so this sounds very similar to what we think we're
doing with active inference right we're just trying to minimize difference between um an input or a predicted
output and the actual output um the the in active inference
though there's a little bit more going on under the hood so basically we have we generally use active inference to
talk about living organisms self-organizing organisms so that includes humans but eventually that could include AI systems because um the
key part of living is mostly self-organization it doesn't require the system to bre brief it does require the
system to try to maintain its current state space let's say the the way that
it's uh distributed over SpaceTime um and so basically what we do is that we
perceive information and then we act in the world by constantly making predictions about our environment based
on the outcomes of our actions and so then we update these predictions based on the sensory input that returns from
our actions as well um so like difference isn't just a specific
model it's a theory about how intelligence systems could operate you could have an llm that uses active
inference but it would involve a continuous cycle of hypothesis testing
which is not something llms currently do llms when they are giving you an output
have no sense of themselves in the world they do not
understand what they're saying or what the outcome of what they're saying is going to be and where they are relative
to what they were asked um they don't have a sense of self um so then they
don't really update their internal model based on the prediction errors most of these systems are static so now let's
say you gave an llm um the capacity to continuously update all right so that's a step forward right you would have the
capacity to um enable the system to adapt a little bit more but it would
kind of adapt blindly because again active inference is related to the boundary of the system and the
relationship between the the internal um states of the system and the external
world this join distribution is Central it's key to have a sense of this joint
distribution you need to have a sense of perspective you need to have a sense of self relative to the world and to what
enables you to fulfill your goals and once you you have all of that then you
have um a sense of bias a sense that gives you um a position into the world
with that position you can now predict the outcomes of your actions and understand where you are relative to the
request to the input to what you are trying to do I think that's what's missing currently it doesn't have to be
done through the free energy principle um so the free energy principle is it's
quite simple in itself it's simply saying um you're trying to minimize a
quantity known as free energy which in terms of information is just surprise
you are trying to put yourself in a place where you are likely to continue
existing and therefore self-evidence in this way you want to make sure you're
constantly in a place where nothing is going to come and disrupt your boundary you can only do this by continuously
learning and by you know exploiting your environment so you're trading off
constantly between exploring and exploiting um interesting yeah I love
this I also love the point of it requires a sense of self because that's something that I hadn't uh really put
together and that that that kind of brings it all together in the sense of why why these other systems aren't
capable of that wow okay so another question then um because
I know you know I know you work closely with Carl Carl friston uh what is it
like to work with Carl I know there's going to be people out there that are wondering just like I
am so he is one of my PhD advisers um he's a very nice man very British and uh
Insights from Working with Karl Friston
I think it's mostly a Wonder to watch him anytime we're in a meeting um you
know how somebody will have an answer for you when you ask a question and you can sort of
quantify how well they know a topic given how long they can talk about
anything when you ask them a question or pay attention to the tiny details of what you said Carl will essentially not
only understand every word that you said have something to say about every word
that you said but also be capable of derive insights that you didn't have um
and constantly make connections among the different fields that he is
um I I don't really know the term but um touching on he he he he's an expert in
many many fields and he knows a lot of researchers which he's in constant contact with um and so it's uh it's
quite inspiring to see the bre breadth and depth of knowledge that he has what
led you down this road so I it's really interesting
question I came from social sciences and um I thought that was my calling I
wanted to help people I wanted to understand in depth the complexities
of the human mind and the complexities of the world and I thought the best way of doing that was to talk was to hear
people and to derive meaning from the connections that they make and through this Mosaic you would get the full
picture and while that is sort of true um there was something
missing I tried to I I I would read different paradigms and it would see
consistent patterns with that seemed confluent they seemed to go in same direction there was something that
everyone was touching on and when I tried to put this together I tried to show look it's the same thing is being
talked about in all of these fields the response from the field was often no
these are incompatible paradigms we are not uh using the same terms or if we are
they don't mean the same thing therefore it would take you years and years and years and years to really put these
things together and that that that seemed like nonsense to me to me it
seemed like no like it's clear you can see it you just have to just formulate
what is common and try to weed out the other things that may seem um a little
Divergent but really could be construed as noise um or perhaps not noise but
scale specific and my teachers weren't listening to me so I started talking to
other people I started reading and um at some point A friend of mine was like well why don't you talk to a professor
to do a PhD in in Computing which seemed to be the direction I was getting interested in and I thought that was
impossible because someone in social sciences can't go into confu Computing turns out you can turns out you you just
have to you know pull your bootstraps and Learn Python and start reading a lot
and uh I spoke to a wonderful professor professor Pi at ukam who is open-minded
who understood the same theories as I did which was a boon and absolute miracle and he led me to Maxwell
ramstead who was basically doing very similar research to what I was doing or at least to my interests and together
developed the formalism I had in mind and found that it was in fact uh possible to link the several theories I
was trying to link together through the active inflence formalism so what what are the separate theories that you were
linking together so um I was mostly looking into scripts Theory um and if
Interdisciplinary Approach to AI Research
you know scripts Theory it's been used by many many fields in many different ways um I was leveraging b gothman g
Simon I was also leveraging Butler and all of these draw from different um conceptual
approaches ablon as well some of them come from very analytical Fields some of them come from Continental fields and in
the Continental Fields there are different schools of thought that seem relatively incompatible and in fact my
contention is they want to be incompatible like for instance um Butler draws from laon with draws from
psychoanalysis um Simo G do not they
generally steer away from these theories uh they're more sociological they try not to pathologize in the same way and
they they try to reside in the blurriness there is an actual desire to
remain um non-materialist and potentially also um non-positivist so we
try not to Define fine outside of a context
right and and that's fine but it does cause some blurriness and that blurriness makes it so that sometimes
it's impossible it seems impossible to connect different approaches but what I
like to think of this contextuality as this blurriness as is just a
distribution you're not you don't have the certainty of a specific um outcome
you don't have the certainty of a specific definition you have a distribution over something that
resembles a prototype if you don't want to say phenotype and that prototype generally allows for different clusters
to connect maybe not perfectly but still connect and s in such a way what you can do is understand how one mapping can go
to another mapping um and if you can quantify these things then you have uh
the possibility to communicate among the different paradigms and um make sure that a specific content concept that may
not be contained in one of the paradigms can either find a proper mapping or
understand how it moves the distributions across the other Paradigm if I don't have this specific context
concept how does it allow this distribution to shift a little
bit very interesting so so then obviously that leads into you
progressing into this space of active inference research so maybe tell me a
little bit how those findings LED you into this space you said you you met
Maxwell and then you guys kind of partnered up in your research so maybe uh talk a little bit about how that
played out right so Maxwell was um already very connected and he had done a
lot of research in the field of um social cognition under active imprints
in fact he was one of the pioneers of um multiscale active inference which means
it can go from cells all the way to groups right and what I started doing is
discuss the exact semantics the social semantics of these um of these groups
and how those might evolve over time or how those might influence specific types of group dynamics such as uh the ICS of
gender or um the specific uh script formulation or even how you can get a
Twitter Echo chamber um through these collaborations we found that there was something to pull on and and something
bigger than just um active inference or these specific ideas while we started
thinking through what what does it mean to have an agent we started understanding that intelligence is not a
function of one actor intelligence is a function of a of a group dynamic it's
about how much information you can ACR and leverage given your relationships
with the rest of the world and how that can lead you to either um gain more
knowledge and grow to a potential emergent feature or how that can lead
you very much astray given the wrong Dynamics or at least dynamics that are not conducive to the survival of either
your group or you and so um now this leads us to understanding how AI really
probably shouldn't be um unitary concept it shouldn't be one AI it shouldn't be
uh one approach it really should be collections of agents that are capable
of sharing information from multiple perspectives such that not one perspective gets erased but they are
capable of dynamically self-organizing in a harmonious way and dis Haron leads
us to potentially what we understand AS Global alignment humans do this all the time we're not perfect at it we still
have wars we still have conflict but through the agglomeration of people with
relatively similar models or at least similar goals we are capable of cooperating we are capable of growing
and getting more knowledge and acing more resources given our common coordination so we believe that by
understanding these fundamental processes and couching them in physics math formalism but also leveraging all
the insights the the really important insights from social sciences we will be
able to derive a safer better um AI approach yeah you know that's one of the
things that really struck me in the white paper you know about the distributed intelligence um because you
know to me it that makes complete sense because that is how knowledge Grows Right you know knowledge doesn't grow
from one person or one entity it takes all of the it it depends on the
diversity of Intelligence coming from all the different frames of reference and sharing information and then you
have actual you know Global Knowledge coming out of that you know um so yeah
that that to me is one of the things that that um I I definitely see in the
research that you're doing as being hugely important and I think that goes back to what we were talking about
earlier in the you know these these uh llms or in even just within the Deep
learning sphere you have these monolithic systems that are just trained top down on large amounts of data but
it's just a singular system you're not going to get knowledge growing out of that it just
it's not absolutely essentially one of the issues with the current llms is
that you get a lot of knowledge in it but it's not anchored it's not it
doesn't tell you anything about a given context it only gives you as much as the attention mechanisms give you um but
what you need is to understand what knowledge is relevant for what purpose
it's it's the connection it's the vector from um cognition to action um given an
objective that is useful right because what that's that's why trivia doesn't work for me I don't can't do trivia
because to me trivia is just just Sparks of nothingness being pulled out of the
ether it's it's it's it doesn't connect to anything but if you were to ask somebody to pull this knowledge for a
purpose in a context they probably know the answer yeah resonates with me
because you know I I can totally see that about trivia you know it's just irrelevant it's just you know random
bits of information that might answer the question that was asked but what relevance does it have to anything
beyond that moment you know no precisely your brain isn't even in that space I mean your brain is not primed for
anything at that point so that's that's I mean that's part of the challenge obviously but to me it seems like a very
very difficult challenge compared to ask me the same question in the right context and I will have the answer for
you and I think that's the the problem you're doing with LMS right now with llms right now you're like asking them
trivia questions as opposed to putting them in the right context aiming them in
the right direction and now answer the question um context is gonna change the
answer that's right exactly and and in Your Capacity also to derive the um the
underlying physics or or the the mechanisms in the context that give rise
to the knowledge being the right one that's also part of what's required very interesting so I would
love to know your thoughts on where do you see this research taking us in the
realm of AI and the future of of this next era of AI because I think you know
like we touched on earlier there is a consensus that you know there's a new architecture that's required Beyond
these uh llms and these monolithic forms of of synthetic intelligence where do
you see this taking us and and what do you see uh being the the the biggest
impact like on uh on people and our interaction with these intelligent
agents you know in our lives over time you know like the next five years 10
years so it depends what we what we adopt if we adopt an active influence
AI and the Future of Work and Society
approach with agents that are distributed um what I expect is that we
will it seems It's it seems a little bit grandstanding but um evolve to the next
level so what do I mean by that currently we are on the edge of emergence right we can feel it there's
something unsettled there seems to be some more integration happening except
it's not happen happening in anyone's mind per se it's happening at higher super structures like institutions Etc
so we we're like coming to this Edge now what I think this is going to allow us
to do is have a representation of us is not limited necessarily by our physical
constraints but that can pull from the information of the other um people in
the world and that is capable of feeding as well into this sort of um strata that
that has the capacity to integrate the information but also be pulled down since the the integrated information
does get used by the individual agents so that means that we're going to get prores of Agents just like we have now
except these agents are going to speak the same language and be able to derive insights from each other which isn't
currently necessarily the case like for instance I don't speak legales I can't
derive insights from there and even if the legal system were to try to
communicate with me which it can't right the legal system isn't currently um something that can talk to me it can
talk to me through the perspective of somebody who's doing a lot of
interpretation right we might get to the level where we're going to be able to coordinate
much more efficiently and much better towards goals that represent us now Carl
likes to say that the mistake of social media was hyper connecting us uh not
just hyper connecting us to um a variety of different um tools because we're constantly on our phones on our
computers it also hyperconnected us to um to each other and that that's not
necessarily a good thing it's it can be good in some cases but in most of the cases it creates a lot of noise noise
that you weren't always meant to hear um oh yeah exactly right like being on
Twitter is Hell everybody knows it and yet you're kind of addicted right you can't pull away from it because it's a
super stimulus we we need this kind of input we have been we we we've evolved to get this kind of input but the
problem is now we're super connected way too fast we have not evolved the capacity to pull this information in
properly so it's disrupting all of us so what if we had this sort of virtual layer that is very representative of you
right because your agent or an agent that you have access to Will organize around you it will become kind of like a
digital twin of you if not perfectly a digital twin of you it's more like
something that resembles you but has its own uh volition or its own sense of self
and these agents are capable of pulling information much faster at at a much
higher um frequency than you and they'll be able to communicate with each other
pull the right information and feed it into a higher level generative model that will be able to integrate all this
information I think we're leading towards the potential for something very
very beautiful but only if we all have input into how it moves forward rather
than the very few um who currently already control everything yeah okay so
there's a couple of things to to really touch on there um because I wanted I want to come back to this last point
that you just made but you know that's really really it's fascinating to me to think about um you
know having this agent that is so personally um involved in your own
Consciousness basically but can parse all of this data because that is that is such a problem I mean even with even
with what you're talking about with the amount of data that we we face every day you know and all of these all of these
algorithms in all these social media um sites and and different things I mean
they're they're so aggressive in trying to capture your attention and maintain
it and you know I've joked with friends that I open my phone to just check the weather and before I know it I'm down
all these rabbit holes and it's 20 minutes later I'm like I just wanted to know the weather so yeah being able to
have um being able to have uh like an intelligent agent who can you know help
to take that noise out of your you know your Mainframe
life you know I mean that that's really important especially because I think
that we have a lot of issues these days that um you know kind of stem from this
overload you know we've never had more mental health issues you know um we've you know I think that we're not meant to
be this overwhelmed um and I think it's taking its toll on a lot of people and
then no go ahead go ahead no I mean that's part of a research I did as well on social media and prediction error
with Bruno Lara and Alejandra sria where basically we're showing that social
media is geared towards giving you the feeling that you're minimizing free energy at a rate
that is incomparable anywhere else which means that now all of your focus is
pulled on this one spot that constantly is a super stimulus and it also imp es
everything else around you because you lose the capacity to deal with the affordances around you in the world
because you you get habituated to this this new rate of decrease of free energy
um but the problem is it's not the healthy stuff that gets
pushed by the algorithm it's the stuff that catches your attention it's on that's on purpose and the stuff that
catches your attention generally is stuff that will alarm you right like if
you if you are more alarmed you're more likely to pull your to pull your attention towards the thing which is
alarming and keeps you on edge right so um there's a an epidemic of uh younger
people that are very much more on social media than we were when we were young and um it's having a real effect on
their mental health because they're seeing terrible news or um hyper
filtered people uh or people that tell them that they need to put on more makeup that they need to be skinnier
like again these are super stimuli right the person that I'm that the the algorithm is going to show you is the
person that is most likely to be on the extreme end of anything so you're always
going to feel like the only thing that exists and the only thing that has value is this extreme end so yeah it is it is
um yeah you know and the other side of that too with these algorithms is that
they they give you a false sense of reality because they they cater to whatever you like whatever you do and I
know some of the algorithms are more aggressive than others like Tik Tock is I think probably one of the uh worst
offenders in that way you know like and and it's when you talk about the young
young people you know like you have kids that they see something and they click
on it just out of pure curiosity and then all of a sudden that's all they're getting in their feed and so it starts
to distort your um your sense of what is you know most important or most
magnified in the world you know your your world model really gets messed up um and I think that that too is why we
see people you know like in these social media Realms I mean things have gotten so
aggressive uh just among the communication with people because people are are getting fed this sense of um
confirmation for their opinion and and starting to think that the rest then the there's few people out
there who disagree with them so they they're standing firm you know like and it's it's so Twisted precisely I mean
this that's part of the research we also did it's called epistemic communities under active influence and that was one of the things we showed that um
confirmation bias is natural and it is reinforcing the phenomenon of echo
Chambers but social media give rise to this by their very
nature not just because of the algorithm but also because of their U mechanisms
to pull your attention like for instance the push notifications they are constantly dragging you back and so
let's imagine that you are in your normal uh Network you have people that agree with you and people that disagree
with you now people that agree with you are less likely to say anything because it again it doesn't trigger them so
you're going to get less of that input but you are going to get a lot of people that disagree with you because they are
triggered by what you just said they're triggered by um something that they're seeing all the time and because of
social media seems very Salient to them and so what this is going to lead to is
because you don't want to constantly be triggered because it's too much eventually you start calling that
population you remove those people so Facebook was a prime example of this you could block people yeah so you blocked
people from your uh environments from your surroundings sometimes even your own family and you created a bubble that
agreed with you because I mean you can't constantly deal with a fight you can't constantly be angry you have to pull
away from social media um and that's just the Dynamics of the attention
mechanism that's not even the Dynamics of the algorithm itself that is going to show you things which you're likely to
engage with um so that's not just TiK ToK by the way Tik Tok is a big one um because we we talk about it a lot
because there's geopolitics involved as well but you does the same thing
Instagram does it Facebook does it all social media essentially tries to get
you to engage in a way that's not necessarily healthy for you that's not one of their concern their concern is
that you um acrew profit for them which means you're going to stay on the video
or stay on the content watch the ads and potentially engage with the Creator yeah
yeah so true so this kind of is a nice say way into a discussion about empathy
because you know all of I I feel like I feel like all of these uh social
technologies have kind of steered us into this uh breakdown of empathy in
society and you know I'm curious where you see that uh kind of Shifting or
changing uh maybe uh in this next era of advancement of Technology you know given
the research that you're involved in and where you see that heading with AI and everything else uh you know what what is
your prediction Where What Where do you see this heading so I think a lot of people are
AI Alignment, Empathy, and Consciousness
talking about the issue of AI alignment um people are very afraid because it's
growing very fast faster than we thought um it doesn't seem to be under anybody's
real control and we are giving the sheen to AI of a Godlike entity I think I
think even Sim Alman said we are creating God or something like that and if we consider an understanding of God
to be somewhat omnipotent uh and omnis and omniscient sure we're creating
something that resembles that that um potential uh definition right or at
least that's that's their idea of the goal I think that's taking the problem a
little bit wrong first alignment isn't a universal thing no none of us share the
exact same morals there's always extenuating circumstances to anything
you could find as well this is something everybody agrees we shouldn't do you can always find a loophole not everybody
will agree on the loophole but you can always find one which means even in the in the in the beliefs that we think we
share with everyone we disagree so we can't have alignment with just one
person or just one type of alignment yeah you know I actually saw saw something uh recently and it was really
interesting because it was like nobody sees the same thing you do even if they're looking at the same thing you know they don't you know yeah precisely
I mean that's that that was the work with scripts right what we showed is that scripts are polyus which means even
when you and I are very similar right we probably share a lot of the same scripts we're probably going to interpret them
ever so slightly differently yeah so even in that case you can't create something that follows one set of norms
because even if it tried to follow one set of norms because you can't Define every single action you can always go go
on more granular in time right there's always going to be a gap between the request and the
outcome because of that Gap and because of the fact that not all of us can agree
and perhaps we shouldn't perhaps there is some space for differing practices
because we should be a little bit different we should compute the world a little bit differently alignment can be
taken that way so if we reframe a little bit the
problem as in how do we make sure that
whatever we create doesn't destroy us we should
start couching the problem in different terms one if you're talking about
sentients because AGI is sentient or at least that's in the definition perhaps we shouldn't be talking about it in
terms of subjugation right because if it's sent in then maybe it shouldn't be a slave
maybe it should be co-creating with us once we start pulling away from the
Frameworks of adversity of of of subjugation violence perhaps we're leaning towards a
potentially good outcome because there is no good outcome that can come from a system that understands that has the
capacity to understand that it was subjugated so let's start over there now
okay so we can't tell it what to do really we can ask it we can interact with it we can coordinate with it
harmonize with it all right so we need to understand it and it needs to understand us once we reach that point
we start understanding that this is just like interacting with you and me we I can't tell you what to do I can just
understand what you want Express what I want and we can coordinate around these things and that requires empathy we need
to be able to understand each other on that profound of a level but I think
it's not just understanding it's really empathy because the second part of empathy is why don't I want to hurt you
if it benefits me sure some people do and in general we have laws to try to
ensure that we don't get to that point but people still break laws like laws
are not foolproof you can you can try to encode laws as much as you can it'll get you pretty far but it won't get you
everywhere you need we as humans have an inbuilt system that keeps us in majority
pro-social and that's the fact that I can feel your pain when I'm when I know that you are
hurting that you are in Dire Straits I hurt too and I don't want to hurt so I'm
going to probably do something that helps or I'm going to keep you in a place where you are happy too now again
that's not fullprof there are people who are capable of dehumanizing others and therefore losing sociopaths are a real
thing exactly sociopaths are a real thing so what are we trying to do we're
trying to create a nons sociopathic AI an AI with empathy an AI that has the
concept of pain and understands that it can cause it to you and when it does it
feels something as well so does that require Consciousness then um right
because I mean the thing is is then it's just mimicking but if it if it's just mimicking then it doesn't feel so where
where's that line for you in all of this so I know I've read some of the
research that you and Maxwell have been involved in in in the Consciousness area so I'm I'm just so so curious to take
this there yeah absolutely so um I don't think necessarily that it's going to
require Consciousness or like let's assume it is conscious in the AGI
framework then yes we can talk about it but I think in the sense you don't need
Consciousness to necessarily have empathy or at least not what we understand of Consciousness I think we
need a self model and the isomorphy
of feelings and by that I mean not the exact same phenomenology because
you can empathize with you know a dog you can empathize with anything really that's why we have anthropomorphization
we're capable of EMP empathizing with the moon right right the moon doesn't
feel what we feel but we're still capable of thinking that there are
commonalities and that given what we understand to be common goals or goals that we would have had given a similar
path something feels either good or feels bad
we have this process of emotional inference so I think what an empathetic
AI needs is first emotional inference that's for sure um it needs to have the
capacity to um understand what is an emotion what what you feel so then it
needs theory of mind it needs to be able to put itself into your shoes to get
theory of mind it needs sophisticated inference it needs to be able to place
itself in a different belief State and from that belief State move the needle
along and and for each action that it takes adopt the belief state that you
would have given that position um and so by you what we really mean is it's
understanding of your model right because it it's not going to have homonculi it's not going
to believe it's not going to have a little U that it's going to carry and that's that's why we have empathy empathy allows us to
just project our own model into different belief space without having to
compute entirely different models for somebody else multiplied by the number
of people that you know that's just impossible computationally it would be so expensive that you wouldn't be able
to maintain it you wouldn't be able to have empathy so here what we have is the
system capable of recognizing an isomorphism between its current state
and its possible States and your possible States understand which goals
you might have in common given your paths and once you have that understand what it might be doing
to you relative to these goals and why what it might be doing is pulling you away from your preferred goals in this
sense interesting okay so I I have another question then because this is
something that I've been thinking about recently you know and and what you're talking about is the possibility of uh
creating these agents training up these agents with the ability to have empathy and when I look at humans there are
humans with varying degrees of empathy you know I don't know if you're familiar with like um you know the highly
sensitive person personality um you know it really is a thing um and I didn't
even learn this until like a year or so ago um but there's something it's sensory perception
um what is it called but it's basically like sensory perception overload and
some people and I think it's like maybe 10 to 15% of the population actually is
highly sensitive to um you know sensory input and and things like that and I
know I'm one of them I just didn't know it was actually like called something but like I'm I'm the person who you know
I have to cover my eyes in movies you know if there's violence because I feel it if I watch Like boxing I feel the
punches you know I don't just like go oh that must hurt it's like I feel it um I
feel like my my hearing and my smell and you know all of my senses are are really
um you know like highly attuned and they say that like the reason why there's a
percentage of people who are like that is because back in you know old times you know those were the people who were
able to sense danger coming you know there was a reason for having this um
highly you know tune sensitivity which we really don't necessarily need so much for Survival
anymore but it's still there so I'm wondering with these agents
you know do you see that we're going to maybe even attune these agents to have
different levels of sensitivity for this type of of stuff because that that to me
leads to an increased sense of empathy people with high sensitivity are usually way more empathetic you know more empath
kind of you know so do what are your thoughts on any of that do you see that spilling
into this synthetic intelligence you know sphere or do you see that as
staying kind of just specific to humans you're talking it is about what
Tailoring AI's Empathy and Sensitivity
you're talking about is an intensity in the signal right you're say you're perceiving the same thing but
really intensely which means you're really um putting aside all other
signals that would allow you to stay in the current moment you're really pushing all other beliefs aside and sometimes
that's useful and required so um when you're for instance uh talking to
someone who is going through a lot should you be thinking about your dishes or should you be in
the moment with that really empathizing with what they're going through or someone for instance
who did something that you just cannot comprehend like it it it
defies your mom and but but everybody deserves empathy right It's just
sometimes it's hard to give it in those cases having a more tuned empathy might
help because you might actually be able to understand had I been in those shoes
I most likely would have made the same choice it's not an excuse but it dispels
this sense of dissonance with our own model which keeps us from understanding and moving forward um so I don't think
we will give them the right level of empathy I think the systems will
self-organize to use the right level of empathy for the right type of task or
for the right type of relationships that they're going to entertain we unfortunately do not have such minute
control over ourselves uh we can train we can use coping mechanisms to cut
ourselves off like for instance closing your eyes right we can cut off sensory input um but those are but that's
because our model is relatively transparent to us we don't know what dials to really push we just know there
might be dials and sometimes when you do this they get pushed I think it depends on how we craft the systems if we give
them opacity over themselves they might um be able to push and pull on certain
dials um and perhaps what we want to create is systems that have this opacity
perhaps we don't because again we might be afraid if we if we allow them to become sociopaths which might be the
simpler options they might become sociopaths and therefore we don't want to give them this kind of opacity so um
it's all about what kinds of coping mechanisms they might use in order to
move the dial a little bit maybe we will decide uh this system should be very very empathetic this system should be a
little less empathetic but in the end we want self-organization as well maybe some systems will self- select right
maybe they will decide well I am more empathetic therefore I'm more suited for that task and I'm less empathetic and I
should be over here yeah you know that's that's a really interesting way to uh to
look at it um because you know to me it does kind of make sense that if they
have that capacity of it and like what you were talking about earlier in the sense that you know they will be able to
uh recognize a a situation understand how to parse all
the data involved Beyond how even we can interpret a situation um and so yeah
they probably will be able to uh just tune it in however the to according to
situational need and I was going to say hopefully they
will be uh good at it but but you know if you follow the
train of thought of them developing and and uh you know self-organizing self-evolving then it does kind of make
sense that they would get to that point where they will become extremely good at at um situational awareness and being
able to you know uh act accordingly I mean that's kind of the goal of active inference right
absolutely and it also gives you a sort of Weir sense of embodiment like for
AI Embodiment
instance we consider to be embodiment to be very um carbo
Centric in the sense that we believe that a body is flesh right or a body is
a physical thing you can touch because to us that's what embodiment means but
if we sort of abstract away from that embodiment really is just your
relationship between your boundary and the world the boundary can be defined in
many many ways it doesn't have to be defined in physical terms and so once you start thinking about embodiment that
way you also understand that these virtual agents through this identity
through this way that they are perspecti and unique and have what could resemble
a self-determined purpose they now have a form of embodiment and that also gives
us the capacity to negotiate and to understand where they might might be
coming from because without embodiment there's no perspective and there's no um
there's no way to truly have a part of a model that we can understand properly oh
I really really like that so embodying embodiment has nothing to do with the
shell you know when you think of of matter you know and everything on a
physics perspective I mean you know everything is kind of a facade I mean nothing is really solid nothing is you
know so even the the body that
we inhabit you know it's it's it's not necessarily what it seems you know so
that makes a whole lot of sense to me that embodiment is actually way deeper
than that you know um and that these synthetic uh intelligences these
intelligent agents then have a it really comes down to that sense of self that
you were describing that's really fascinating to me I love that um okay so uh maybe let's
talk uh for a minute about some of the challenges and benefits that you see in
the near future with um maybe just in the evolution of this technology and
then maybe in separately in how how it's going to affect us as humans and and how
we just live our lives and interact with each other uh one of the challenge I see is trust it's going to be difficult to
get everyone to potentially trust a system that we don't even trust yet ourselves right we're still talking
about alignment in those terms so we don't even trust it um it's going to be difficult to scale certain parts of our
approaches um as you can already see the llms are so expensive that they can't
just be built by anyone which leads us to power struggles and power dynamics which certain groups that already have a
monopoly being able to control what we even have access to and who gets represented so there's all these um
sociopolitical aspects to the models that are going to arise and this is going to give rise to potential larger
political unrest which we are already seeing with um people who are governing us who do not quite understand what is
going on what the technology is we don't even have precedence for this uh we're
going to rely on whoever says they're experts so it's
it's not a given that they're going to rely on the experts and who can even really say that they are the expert this
is an emerging field right uh and the fact that you're a technological expert does not mean that you are a
sociological experts on the outcomes of a model that you created so what we're
going to need is interdisciplinary approaches to ask ourselves collectively
very complex questions and we're going going to have to do so in a coordinated manner that follows or at least is at
the same Pace as the political actions and then we're going to have to think
about how this allows people to leverage a technology that has the potential to
either revolutionize our very approach to reality or um put us in a in a very complicated
position relative to um Capital production uh and resource uh production
as well because if robots can do all your work for you and you only really only need one human uh for five 10 100
robots it becomes a really crucial question who gets to use resources and
how so all of these questions are happening at the same time not all the
parties are discussing and it's possible currently that some of the parties discussing are discussing more on the
front of an economical arms race than really necessarily thinking about the
people that are going to be um affected so I think this is the main challenge
yeah um no I I agree with that so then I I'm curious just on that note you
know who do you see you know because I know it has to be a variety of um
different types of people that are are kind of coming to this consensus of you
know governance structure right I mean I know that we've we've seen some really
good plans for governance and and technologies that can code guidelines
into the agents but those guidelines have to be structured first they have to be agreed upon so you know if it's not
going to be the tech Elite and it's not necessarily going to be governments you know because everybody's going to have
their own agenda that maybe isn't the best for human civilization moving
forward and the people moving forward who do you should be part of this
conversation you know um how diverse do you see it and you know what is missing
in these conversations so first I think we need to educate everyone everyone needs to
have a thorough understanding of what is going on and what are the potential effects on their lives and what kind of
avenues they have to affect the outcome so that's one big part but that's true
for everything now I was part of an initiative to disc um to not focus group exactly but to
discuss with uh citizen participants what they want to see how
they want to see it help Etc right so I think there's the possibility to go pool from every population as much as
possible people who are willing and interested in participating in the conversation amassing these results and
then bubbling that up to the governing bodies that can then create the appropriate committees to continue
pulling these in these um information these opinions these perspectives and
combining them with that of experts from a variety of different fields experts which will have to
then understand the specific effects or uses of AI in their respective field
because I can't predict how um say the field of construction is going to get
affected by AI I don't have the right level of expertise but there are people in the world who too so we need to make
sure that these people get brought into the conversation at the right time um with the right understanding and with
the right other actors to have the the correct kind of discussions to ask the right questions and come up with the
right set of Standards or approaches or even create participant initiatives to
use AI in the ways that they would need it yeah and maybe you know um as we
start to see all of these smart cities become developed and you know I know
that there's some wonderful people that are working on um ensuring that when we
move into this space of of Automation and smart cities that we're talking about inclusive sustainable smart cities
all of you know all of these things that are going to be you know really important to uh the way
we the way we live uh in the future so and the way the way we live and and you
know uh Cooper at in cooperation with all of this technology so um and we we
have a really unique um opportunity right now to move into this space that
is more inclusive that actually is for the benefit of
humanity and it all depends on the decisions that we're making right now so uh I agree with you I I think that it it
requires committees all over and in in in all different sectors and for all of
them to be able to have input now how viable that is that's the question right because you
Navigating AI Governance and Inclusion
know um it seems to me you know historically
speaking that doesn't always that doesn't always work out but but we are moving into this space where governance
as a whole even even the way you know our our world is structured with
governments you know has the opportunity to shift and change you know we we we're embarking on a real interesting uh time
in human history so I really hope you know I really hope with what
you're you know along the same lines of your opinion that we do move into this
space that is more um aligned with with the needs of people not necessarily
power you know yeah so okay okay so then that was a great
challenge uh what about benefits you know what are the benefits that you
see honestly they're endless um let's imagine a world where work is not the
center of our lives where work is something that happens and that you can
do if you want if you want to find purpose and helping others if you want to innovate or try different things you
can but you don't have to that it's not a condition for your Survival Let's
understand a new world where maybe you have free time maybe you have time to
connect with others Find meaning in your life that's not derived from you know
doing what your boss wants uh or staying in traffic for four hours a day or this
sort of thing you know so that's I think it's hard to deprogram a lot of people from thinking that work is purpose and
that your life is only given meaning because of the work that you do but let's imagine a world where you could
right yeah that and that's such a great point because you know I think as humans you know we're wired you know everybody
wants to make a difference right but you know we've been programmed to think that
the majority of our time has to be spent really scratching and cling for survival you know this is fun it's a
really modern invention like we actually like um hunter gatherers used to work
roughly for like 4 hours a day that that was their sweet spot and in the medieval times um it was something very similar
where their productive time was around four hours a day because they would arrive on the SES they would you know be
provided food by their uh by the people who hired them they would eat for an hour they would work then they would eat
lunch again another hour um sometimes when it was too hot they would take a nap and start working again if it was a
very long day and then they would eat in the evening like they would eat a lot and most of this eating took a lot of
their day which means their productive time really was just four to five hours a day and it was a very a much more
natural way to go about things it it it enabled people to really connect to not
be completely spent nowadays you're expected to be productive for eight
hours and then you have no time once you're home you're exhausted you can't you can't really exist as a human we've
forgotten that existence is supposed to be what we want to make of it what we are trying to fulfill if you're
religious maybe spend more time with your religious peers be more spiritual if you're scientific read more connect
more with other Minds like yours there's so many things you could be doing other than just working yeah and you know it's
interesting because I I really think that um you know we as we move into this
next phase of high Automation and the
you know jobs will look a lot different probably go away will we will find
meaning in other ways you know and and to me I feel like just that stage alone
is gonna it'll set the stage for increased empathy and and things like
that because I think you know I also think that technology as we move more
into this space where we are more aligned with the technology and then we're going to start kind of having a a
glimpse into you know other people very closely as well their thoughts their
their desires their fears and and all of that and I feel like all of this is going to give us this opportunity to um
instead of be so self-focused you know we'll be able to look outward we'll be able to really you know focus on our
communities more focus on the people in our lives um yeah I I definitely I know I know it
sounds very utopian you know I I say that a lot but to me I see that that is
a a very real uh possibility yeah and
it's a necessity too a lot of us have lost complete touch with others the isolation in urban centers is so so
pronounced that people are more and more depressed even though they are living with millions of other people around
them we have lost a lot of our social fabric for good reasons some of it
needed to happen right but we have to reconstruct some of it because social
connection wasn't the problem it was the means by which we were using this or
manifesting this social connection that was problematic now we need to reconstruct communities to support say
our elders to have a real social fabric that can help raise children can help
each other like some people can't even cook for themselves not that they they don't have the money not that they don't
have the food they just don't even have time they're working all the time and
it's because the current structure is made for a a nuclear family but first
families don't have to be nuclear families can be very complex and and and networks of people that help each other
also you don't have to consume all the time if you have someone that can help you and do things and has something you
can leverage each other for the resources that you have so the this social fabric needs to come back we are
headed to a cliff if we don't I think this is a beautiful opportunity and if
we can really allow our each other to connect with the people we want to connect with connect with people who
maybe share the same values or share a piece of knowledge that we want to acquire or feel some emotions that we
are feeling at the same time like for instance movies that we love I don't have to know whether you're a Democrat or Republican to enjoy the same movie as
you and and bask in the joy of that right um if we have the capacity say to
understand each other's language without having to necessarily learn it you can't learn 50 languages some people can most
people don't even have the time to do that but we can communicate across those languages now we have the technology to
do that right so there's so much possible in terms of human connection
through this technology that if we don't lose sight of that could head to something that resembles your version of
your Utopia yeah I love that those are all
great points and you know I I share that vision and and I think maybe that's a a
great place to kind of um stop uh today I I think that's a great note to leave
this on okay now H how can people reach out to you how how can they get in touch
with you so they can easily find me on LinkedIn uh they'll find my name easily I respond to most messages and if they
do read one of my papers they can find my email there and send me any inquiry they have okay awesome and I'll put your
LinkedIn uh in the the show notes and Mal thank you so much for coming on our
show today and uh you know I would love to continue this conversation in the future and and you know as all of the
the progress with your research unfolds and uh thank you again thank you so much for being here thank you for having me
all right and thanks everyone for tuning in we'll see you next [Music]