https://www.youtube.com/watch?v=n8G50ynU0Vg
The Myth of Pure Intelligence

Machine Learning Street Talk



00:00:00- Intro / IIT scandal 
00:05:54 - Gaydar paper / What makes good science
00:10:51 - Language
00:18:16 - Intelligence
00:29:06 - X-risk
00:40:49 - Self modelling
00:43:56 - Anthropomorphisation
00:46:41 - Mediation and subjectivity
00:51:03 - Understanding
00:56:33 - Resiliency

Technical topics: 
1. Integrated Information Theory (IIT) - Giulio Tononi
2. The "hard problem" of consciousness - David Chalmers
3. Panpsychism and Computationalism in philosophy of mind
4. Active Inference Framework - Karl Friston
5. Theory of Mind and its computation in AI systems
6. Noam Chomsky's views on language models and linguistics
7. Daniel Dennett's Intentional Stance theory
8. Collective intelligence and system resilience
9. Redundancy and degeneracy in complex systems
10. Michael Levin's research on bioelectricity and pattern formation
11. The role of phenomenology in cognitive science


I should look into your eyes. Um, a bit of a bit of a combo. So. Yeah. Like that camera vaguely in my
direction. I don't think it matters. Okay. Just don't look down, okay? Don't look down. Uh, because of the the glasses
and the light. Oh, yeah. Let me check that. Tiny bit sorted.
But when I say "saucy", I mean "light source"!!! So I'm director of product for R&D at verses,
and I'm also a student in PhD for cognitive computing at the University
of Quebec in Montreal. Fantastic. So what have we just been discussing?
God, we've been discussing a lot of things. Um, I think we discussed panpsychism most recently.
Uh, we discussed computationalism. We discussed whether we should be focused on materiality or whether it's about the processes themselves.
The hard problem, it lots of things. Lots of things. Yeah. So why don't we start at the beginning?
So that's quite a good segue actually. So recently there was this letter of opposition, uh, you know,
around integrated information theory. What's your take on that?
I think the reaction was to be expected. I think you can't ask people that aren't involved in a process which is
aimed at determining whether they are right or wrong, just to accept
the outcomes of something in which they had no say. So there's that. It's like saying, well, you're going to have to accept the outcome of
elections, but you can't vote. Um, I think one of the issues is they
the people who created the study determined a process that left too much room, too much wiggle room for interpretation and critique,
which means there's a problem in the study design to begin with. Um, I think when you start attacking through adversarial means of field,
it is highly likely the field is going to retaliate. So this open letter, I think, was a retaliatory move.
I think it was a very strong response to perhaps coverage that
should have been more nuanced. I definitely don't think it is the leading consciousness theory. I think it's an interesting one.
I think it has promise. I think it's definitely formal. There are many others. I think this one was just one of
the ones that was selected in the Templeton study. And so, um, I think we need to be more careful
about how we comport ourselves towards other scientists when we call an entire field pseudoscience. I don't think it helps.
I think we can critique the parts of the field we think are, um, egregious or problematic. I think that's helpful because it
builds towards something, rather than just destroying something for which a lot of people have given their lives. They've not, not through death,
but like they've given a lot of their time. They've studied, they've they've understood what
would constitute the right kind of methodology and they've applied it. So I don't think we can just disqualify it as something that
has no value offhand. Now, it's not to say that they necessarily all had an offhand opinion, but the way it was
presented, um, basically suggested because of tabloid style reporting,
uh, we're, we're leaning towards something dangerous. And to do so they used tabloid styled methods. So that's that's my take.
Do you think there is any bright line between what is legitimate science and what isn't? I think there might be, but I don't
think it's where people think it is. I think, for instance, um. Some people called science, uh, something that used tools.
And I think to me, that's insufficient to be to qualify as science. So it's not because you use a little
penny penny machine to, like, go like this on somebody's head, that phrenology is effectively a science. That's not I think you need to,
um, measure invariance and measure how those invariants allow you
some degree of predictability. To me, that's the. Um, next, the essence of science. But I also think that some
things that people can, um, discard as evidence still can qualify as evidence. To me, phenomenology can become
a part of evidence. Uh, to me, um, intersectional data, which means it's not necessarily data that will easily reproduce,
can qualify as data under the caveats that it is not something
that would reproduce, that it is something that is specific to a perspective and therefore cannot qualify as a generalizer.
I think a lot of qualitative methods have developed entire frameworks
to explain how to interpret what they measured as data,
and oftentimes because people in the Stem fields are not aware of how this
interpretation is meant to be taken, they just see something which doesn't seem very systematic. And they're like, well,
that's not data. I was like, well, okay, to your measurement, that's not data, because there is no way for what
you're using to interpret that in any sort of systematized way. Sure. But for someone who understands how to generate that data,
how to potentially return to the space which has similar, uh, um,
partitions and how to turn this into something which becomes a little more systematic, like thematic analysis. Um.
And let's say inter judge agreement, then, you know, we get to something that resembles Systematicity and therefore we
can derive patterns from there. So I think, um, the line is not as
bright as people would like to think, but it does exist. Yeah. I like your notion of systematicity. Um, because I suppose you could
Gaydar paper / What makes good science
just argue on some kind of just basic utility. You mentioned phrenology, and there was this horrific
gaydar paper a few years ago. I don't know if you remember where they ingested a bunch, like a whole bunch of images from dating websites,
and they trained it to predict whether you were gay or not. And it was from Stanford. And the same Stanford researcher
recently published a paper about, um, theory of mind and language models. That's an interesting aside. But anyway, that's something
which is ethically ridiculous, and there are all sorts of confounding factors and so on. But they might argue, well,
this is science because it has predictive power. Well, what part of it was science like? What model did it allow us?
What understanding did it allow us? Did the this the AI have predictive power or did we? And under what circumstances like
did it predict a very accurately whether someone was gay or not? I don't know, but you could tell me whether yes it did or not.
But what does that tell us about the world? Does it tell us that there's a context under which certain sets
of features or common for a given population? Possibly, but I still don't know what those features are.
It didn't really help me. And then the, the, the idea that we can turn certain kinds of studies into, in praxis, I think,
is not beyond the scope of science. That's not really it, but.
I think it requires the scientists to take a stance before the scientific experiment to say, these are the outcomes I believe
should be put out into the world, and therefore this is what I choose to reify with my science. Um, and also the kinds of methods I'm
going to use are themselves anchored in some kind of contextuality. Right? Like the idea of, um, breaking apart the world into certain
kinds of categories which are laden. And I'm going to measure those categories with certain kinds of things, like IQ tests.
It's kind of it's quite, um, controversial. Controverted. Right. It's like, um, do they measure intelligence or do they measure a
certain kind of aptitude given a certain kind of context, like maybe.
And also, what do we mean when we say intelligence? Like, are we saying some people therefore are not and therefore x,
y, z. Um, so I think um, that doesn't
make it phonology though that, that still could become science. And in this in the case of the if the AI model, if it had accuracy,
given that, it probably did. But I don't think that tells me anything about like a causal factor that I could then derive and,
and build upon. Right. Like it doesn't really help. Yeah. No. It's interesting. I mean, like the whole, um,
endeavor of science is something that fascinates me. And once you get past the question, of course, of, you know, well,
what makes good science? Chomsky, by the way, says that, uh, language models are not a theory of language because they basically, you
know, they can recognize anything. You can just put noise into a language model and it will recognize it. So it doesn't really say anything about the why and the what would
happen if not question it just, you know, it just models absolutely everything. So science needs to kind of create
clear dividing lines between what things are and what things are not. And why they happen. Yeah. So I think I just, I just think we
have to be careful when we say what things are and what things are not. I think it's always relative to something else.
Like, um, I think it's true that you can put language in a language model and it will do something, but will it do what you wanted
it to do? I'm not sure. So in the case of Chomsky, I don't think he's wrong. I just think that in general what
we mean by language, we don't just mean a way to transmit semantics. I think we mean a way to transmit semantics, which are anchored into
something for which we expect a certain kind of outcome to,
to also follow. Because if both you and I mean blue butterfly, but we don't mean the same like referent.
Um, then this is useless. That's not really language. We're not we're not really like converging over anything.
So I think that's the thing. Like the language models don't necessarily converge with us where we want them to converge.
So in that sense they're disconnected, but they do to me. And you carry some semantics. And that's how we can tell that,
like they're not doing what we thought they were going to do. They're they can get disaligned with us. Um, so yeah.
So again, like things that they what things are and what things are not is always relative to some kind of perspective that
frames it and constrains it for it to be at all. Yeah. I mean, because obviously we can get to the question of whether, um,
Language
you think language is a system of thought or a system of communication, but I kind of agree with what you're saying about language models,
which is that, um, essentially they are spitting out words, which means something to us, and we can discuss where that
intrinsic meaning comes from, whether it's pragmatics or whether it's just some kind of colloquial, um, usage of the word or whatever.
I mean, what's your take on on the first question, which is, do you think language evolved as a communication or thinking mechanism?
I think we're baking into definition when we ask for either like is language something that is simply, um, reference between some kind of
cluster and some kind of real thing. Is that language, or does it have to be, um, something which is externalized
as a symbol, in which case then it's purely communication, right? So whether I think we have to be more clear about what we mean,
because I do think that you don't need to externalize what you understood in order to cluster it. It's better it allows you to confirm
and act on it, and especially if there are more agents around you. I think it makes like thinking much more efficient.
Um, so I think if we went all the way down, I think we'd find that
maybe the egg or chicken problem, like, we could solve it sort of, I think at the layers at which we would recognize something that
resembles thought or resembles language, they're no longer separate. Um, but if we cast language as a system of, uh, you know, reference
and referent, then I don't think you need communication for that.
You just need one entity to recognize something and, and be able to keep that as, you know, something it can refer
back to later. So it towards itself. But that also could be qualified as communication, communication over time with yourself. Right.
You you cash something out which you will pull back again afterwards. So whether it's spatial or temporal or spatial temporal I think is the
real question we're asking here. Okay, okay. Interesting. But I think that you can embed thinking inside language.
I mean, for example, um, like if you're writing software, there are design patterns and there are patterns for doing
lots of different things, and we give them names. And then the developers recognize those patterns and the software and
they say, oh, I understand now. And loads and loads of these thinking primitives I think are kind of, um, culturally embedded in our language.
And they're like memes and they get passed through generation, and we just point to those things, and then we,
we link them to those kind of cognitive programs that we have. Um, the fidelity of that mapping, of course, whether it's mediated
or whether it's a simulacrum, we can discuss that. Okay, I don't disagree. Yeah, I think you're right, I think,
but I think you you tapped into this idea of, um, whether this is
thought or does it require a name, would the would the pattern exist without the name and would it be referable without the name?
Like, could we still point to it and then like, um, if we think of,
uh, sign language like. What qualifies as a symbol in sign language? Is it a reliable action that
follows something else? Is it is it this intermediate and in this case, like what qualifies as an intermediate or intermediary
when you're pointing at something? So it is it the fact that I look at you, you look at me, I know you're waiting for something,
so I'm going to point to the thing and you're going to make that connection as well. Is that language? I would argue, yeah, it is language.
And that's why I think animals, to some extent, a lot of them have language if they're capable of transmitting information through
themselves as intermediaries. Um, and again, do you do this with yourself, uh, without having to, to use what we would qualify as, as,
as, you know, words, I think, I think words are just reliable intermediate
clusters and whatever those are. Yeah. No, I completely agree. So yeah, first of all, I think when when we say language, people assume
that we're talking about words. And as you rightly said, just imagine a hypothetical situation. You were born unable to make any
sound. You didn't have a mouth, and you could only write with a pen, and you would still be every bit
as capable as any other human. Or perhaps you could only use sign language, and then you could say, well, does sign language have
the representational fidelity or cardinality of language? Yes it does, absolutely it does. So language is isn't so much about
the cardinality of of I mean, language is just basically a set of strings by the way. So how much can you represent?
But anyway, I think it's about um, kind of social complexification in our brains. And even the ability to write is
quite an interesting example of that. So this is a proto ability that we've learned. And you mentioned animals and
animals don't have the ability to, um, you know, they don't have the flexibility that we have. So they don't have the ability
to map symbols to a variety of different phenomena. They do map some symbols to phenomena, but that seems to be
much more hard wired. Whereas with humans it's very, very flexible and it's a real mystery. And unfortunately we don't have
the phylogenetic record, the intermediate record of how and why that capability developed, but it did develop in humans.
So I can't speak as to to why it developed in humans and not in animals. I think what we can talk about
is what qualifies something as a language. If it's not the words, the sounds, the writing or the the signs.
Um, I think we're talking about a process, which is its degree of rigidity. On a certain scale. So when you and I use words,
there's a very high likelihood that we refer to some similar abstractness
that can be contained right under some kind of Markov blanket, whereas the porosity of me kind of pointing over there,
that could mean a lot of things like I haven't really reduced entropy. Like given a context, there's a possibility I would we
would reduce it. Right. And that's precisely the sociality you're talking about. We're talking about these scripts
that basically, um, constrain, constrain, constrain, constrain. And then within those constraints there are um, a lot of different
kinds of signals that have come to take on a reliable meaning.
And we're so expressive because we have so many of these symbols and we can compose them. So there's different scales, right?
So if I say, um, the word trout, I'm not sure. You know what I mean here.
Like, why trout. What what are you trying to say? But if I add more words, then suddenly I've also added more context to trout. So I've, I've contained constrained.
And now suddenly scales of meaning arise. We have more priors, more priors, and now trout takes on a whole other meaning. Um, but if I kept pointing,
I don't think there'd be lots of information that's contained here until maybe I do the thing for you. And now you'll know maybe that
if I point over there. That's. That's what I mean. Yeah. Yeah. Fascinating. You mentioned intelligence before,
Intelligence
by the way. So what's your what's your take on intelligence to me. Intelligence. It doesn't start as fundamentally
human, right? So we start for that. Like, to me, intelligence is a is a type of process. It's the capacity to derive as many
paths between two points as possible. Mhm. Um, because it entails that you can compute those paths. Right.
So your capacity to compute them is one thing. Uh, and the precision with which you'll be able to reach the point.
Right. You could think you're computing the paths, but you could be wrong. You could actually get to a way different point. So I don't think that would
qualify as intelligence. Intelligence is both, you know, your capacity to reach accurately the point, but also to reach it
through a variety of means that you can entertain at a given time. Okay. Um, yeah. So on on that and we'll talk
about goals in a minute. But let's say a goal is an end state. So there's a state action space and we're traversing through the space.
And then we look at the cone pattern of traversal. And the shape of that cone tells us a lot about intelligence.
So the cone might incorporate future affordances which are not
yet available to us. Um, they might include maybe there's some kind of a cost function because some affordances
are harder to take than others, both because of physical difficulty and intellectual difficulty. So you're saying there's some kind of
utility function that we can sum over all of those trajectories to tell us how intelligent we are? Yeah, I think so.
I think I think it's not necessarily also your capacity to enact them. Right. I think um, because if it was
your capacity to enact them, anybody disempowered would not be very intelligent. And that's not true. That's not true.
They could compute it and be like, well, I mean, I can see that if I were in that position, I would be able to or I can see that, um,
these this is a path that exists, but there is something in the way like, I, I can't go there. That path exists nonetheless.
Like, I know I can go outside right now, but all the doors are closed. So I know I could try to find a way to open the door,
but if eventually the cost for me to get outside would be so high, I would effectively dissolve. So there's no point,
so I might as well just not do it. The likelihood of me actually taking that path is really low because I'm disempowered.
I'm not less intelligent. I found a way. I found a highly improbable way. I just don't think I should do it.
Um, so I think intelligence generally correlates with your
capacity to reach those goals and therefore accrue more resources because you'll be able to, you know, find ways to get towards
places which are self-evidencing. And that also has a self-reinforcing cycle where the more likely you are to accrue to resources,
the more likely you are to find more paths that lead you to more goals, etc. and then maybe, um, if you can stop thinking about the
lower level paths you have to take, then you can compute larger scales and you can become more effective on a, on a again, grander scale.
So you can start influencing more people. You can start, um, thinking in terms of longer time depth.
Um, whereas if you're constantly trying to survive, like everything's constantly like, um, hitting you from everywhere,
then you have to compute all the paths to avoid all those things. So again, privilege plays into the degree to which your,
your intellect can expand, right? Or at least expand in terms of empowerment. But that's not to say you're not
intelligence, it just means that your intelligence gets carried and echoed in the world. And then there's all the social
stuff, right? Um, because you and I are constrained by socialness or by some degree of, uh, social scripts,
we can compute more. We don't have to wonder how we should use that table. We don't have to wonder how I
should use this mic. I know. So, um, the more we have these,
help me to stop or carry on, the more we have these social signals that allow us to compute more, um, the more we can think on larger
scales or deeper time depth. But sometimes I also think that some degree of constraints also sometimes preclude you from thinking that,
um, a potential path is probable. Yeah, yeah. I mean, there's a few things there. I mean, first of all, um,
I quite like this idea that the, um, the length of the, um, prediction
horizon is a good indication of intelligence, but the only way that we can increase the prediction horizon is through creating very
low resolution abstractions to represent our sense world. So that has a kind of brittleness associated with it.
You said something interesting before, which is that you could think of it almost as overcoming adversity or overcoming cost,
which, you know, like so. So some trajectories are going against the grain, but some choose to pursue those trajectories,
and therefore you could think of them as more intelligent. Yeah. So, um, it's quite interesting. I mean, I also like to zoom out and
think of the actual goals themselves. Now, first of all, I think that
goals are a system property. I don't think individual agents have goals, so I'm not sure it makes sense for an agent to pursue a goal.
But let's just, um, as a best faith argument, take a goal as a future
state which an agent is aware of. I would also factor in the utility of those goals and the selection of those goals as being
part of intelligence. Yeah. So again, like, I mean, you said something interesting. It's a property of the system.
And I think if if we take what I said at, at, at faith, um, intelligence
is a property of the system, not just the individual. Right. Because again, you get carried, given that the system props you
somehow and echoes your it gives you capacities to think farther. So given that you and I are in the same system and have the same
relationship to the system, we could quantify our intellect given how far
mine would go relative to yours, but that's only given that we have the same relationship to the system, which is, again, highly unlikely.
Um, you would have to basically be a twin with practically in similar life to have the same relationship to the system.
Um, and then again, I mean, this idea of goal with utility, it's it's always relative to a scale, right?
So like given this scale, I there's this gate, which is highly likely, and this gate, which is slightly likely.
And so I'm probably going to go from here to here. And how I get from here to here is. The real question and the degree
to which I minimize free energy by going by going from point to point and improve my model. Also by going from point to point,
which is going to allow me to reach the next goal without too much energy. So I don't think that, um,
someone taking a path against the grain is necessarily means that they're more intelligent. I think that if they can take an
unlikely path, which also allows them to reach a goal that others
would have wanted to reach and maybe couldn't, or would do so without having to spend more energy at a given scale, means
they're probably more intelligent. But it doesn't mean that if you follow the path, you're not more intelligent. Maybe this is the smoothest ride. Maybe there's no point in doing
something different. Maybe to you. The, um, the goal is, is that simple?
So you took the smoothest ride. That's very smart. You just also were able to compute many other paths to go there,
you know. Yeah. But I think, um, a goal in because we were talking earlier about, um, complex categories like a chair,
it's very, very difficult to define a chair, you know, and using the affordance is the way that most ontologists use, like it's a thing
you sit on or whatever. And a goal. Um, we're not actually talking
about a state, because if we represent the world in this, you know, systematic way where there's we discretize everything,
then if you think about it, um, there would be an infinitude of goals. So we come up with an abstract notion of a goal.
And now we've just got this big goal thing that we, that we want to get to. And still there's an infinite
number of trajectories that would, you know, reach an a state which satisfied that goal. So it just it feels I don't know,
it feels kind of vague to me to say goal. Yeah.
Well I guess like because we were talking earlier about science and the benefit of science and I think like, um, you know, abduction is a key part
of science and abduction is, is about finding a reasonable explanation.
So, you know, selecting out of the infinite set of hypotheses a reasonable set and mapping to them. And it's a similar thing with,
with a goal. Right. So what you're actually doing is you're it's very creative. You're you're basically creating
something from nothing. And you're saying these three things seem very valuable to me. Now, we know that we're talking
about a multi-agent system and the intelligence. And what you said really resonated with me. The intelligence arises at the system level.
So the individual agent, um, I think the goals are not arbitrary, but not a million miles from arbitrary. But when you multiply that with many,
many agents in a large collective intelligence and you kind of average over the top, then you get these emerging goals
and you get intelligent behavior. But it doesn't make sense to me to think of an individual agent as having any meaningful form of
planning and reasoning. Well, I guess I guess we're putting a lot in meaningful here, but I, I just thought experiment.
I like to think of, um. Someone really, really, really smart who would compute like 3000 years into the future.
And you would think like, well, that person would try, you know, solve world hunger and they would like, uh, I don't know, they would
try to take over the government and, you know, they would they would find the path that makes sure that somehow they reach immortality and their
children and something like that. And I'm like. I don't think so.
I think someone really, really, really intelligent would probably.
Be able to compute all the paths and find that they're already on
the free energy minimization path. And they would just, you know, live a very normal life, like they would they would live the
life they seemed to have been set on, and they would continue in that because they're part of a manifold and they're already part
of a of a pool of negentropy. And they, they're already like minimizing somehow as, as Friston says, um, he argues
X-risk
there's something fundamental about, you know, if we talk about utility, for him, utility is existence. And there is an argument that
some things are so fundamental, so platonic, that they go without saying. So maybe it's existence. I'm not I'm not so sure about that.
I still think that inside Friston's framework, there are still many potential ways to act and live. And the X-risk people, they say,
well, obviously these agents are going to seek to maximize power and well, why is it obviously I'm not not entirely sure.
The agents as in the the eyes. Yeah. The the eyes. Well, first of all it's interesting that we're saying plural. Right.
Like in general the well for them actually it's, it's that's right. They're still at the um the eye stage of their being one. Yeah. Right.
I know it's so because they, they haven't, they haven't you know, they haven't got to the, to the Maltese scale yet.
No I know, but I think it speaks to something it speaks to, to like this, this notion of individuality and disconnection from the others.
Whereas no intelligence is a function of your group, not just of yourself. The genius myth is a myth. They all collaborate and prosociality
leads to something better. So I do think that should eyes take over, it won't be one, it'll be several.
It'll be a group, uh, like coalition or something. I'm interested in exploring various limiting factors or, you know,
scaling laws for intelligence. And one of the big ones is knowing. And in a certain environment, there are only a finite number of things
to know and what you know might be related to, to the agents that you cohabit your environment with. But you did say earlier,
when we were having a discussion that you thought in principle that there you could scale intelligence. And I actually like to to
rephrase what you just said. So you said when you zoom out and you're looking not at individual trajectory volumes,
but you're looking at kind of like the adaptability of the entire, um, superorganism, because that jives with the common
definition of intelligence that it's about adaptation efficiency. And I really like that. But I guess I still want want to say,
though, that the x rays people think that there's a pure intelligence. So it's pure reason it's not in any way constrained by material reality
or like, you know what we know. And I think, well, let's look at an ecosystem. And like even even if the agents
had a perfect model of each other. And so they there was a lot of complexification of the models of every single agent.
Surely there's a hard limit of how many time steps they could predict ahead. If that intelligence is pure
like not limited by materiality. Like, say it's not. It doesn't relate to Landauer's principle.
It's not bound by quantum physics. Like, like, is that what you mean? Well, let's let's say it were possible for an agent in a
collective to predict many, many, many, many steps ahead. In order for that to happen, the agent would essentially need
to have an internal simulation of the of the entire system, and maybe not the entire system, because we're speaking about
mediation and interfaces and affordances and so on. So there's still a lot of information hiding there. But let's say the agent had some holistic understanding of the
entire system and could roll trajectories far into the future. So but I guess my point, though, is that the agent wouldn't be
intelligent. It would have memorized everything there is to know about that ecosystem. I, I don't think this time limit thing, I think you would just
lose granularity. We, I think we have hierarchical models. And if you extend this to infinity.
There's a really sort of high level, practically near infinity state,
which you can predict right there, constants to the universe. And so while I can't tell what you're going to be doing in ten
years with precision, I know there's a day you're gonna die. Like there's some constants that I can predict.
Um, uh, you know, I know that, like, it's going to keep traveling until it
no longer, you know, like there's there's a degree of granularity
that can maintain accuracy even if it loses out on precision.
So that's probably one of the solutions you would start losing out on granularity. So if you're asking me,
can you continue predicting with the same level of both precision and accuracy across infinite time steps? I think if you're stable enough.
Yeah. So the question is really about the degree to which you have volatility in your system. Right.
Because if you and I just freeze in time I mean we freeze in time. That's it. I've predicted you for infinity.
Um, so it's it's it's it's really a degree to which you can, um, predict emergence, I think relative to, to volatility.
And I think we'll be able to not not necessarily that soon, but I think it's coming. We're going to be able to
predict emergence. So if we do if we can predict emergence and we can have models that can get to let's say minimal
description length, and they can improve their models such that they can always reach a higher level of a shorter description length.
Then, you know, I think we can predict in infinity. I mean, there's a few things on that. So first of all, I think that,
um, when, when when we have ecosystems of life, there is a
degree of irreducible complexity. And I think there's a kind of power law distribution where like, you know, some of it is reducible
complexity and quite regular. And then like a lot of it is so chaotic on the long on the long tail that it's essentially irreducible.
But I do think what you said is very interesting about there being certain trade offs about predictions. So yes, we could create low
resolution models that could predict far into the future. And we were having an interesting conversation about AlphaGo,
because I think what that does is it's still a high precision. It's a high resolution model. Um,
and even in respect of time steps. But what it does is it's not a robust
model. So it's a pointillistic model. So what it's saying is like, okay, well go has a structure to it. Yeah. Because, you know, the state space
is, you know, combinatorially large. It has a structure to it. And most of the humans traverse the structure along these joints.
Yeah. Yeah. And like I'm going to just like fill you know, I'm just going to fill in the thing around the structure. And that's kind of what you're saying.
So I've got this model which like represents, you know, the trajectories on the structure. There's holes everywhere. Right?
No, I mean I think that's exactly it. I think you're talking about the degree to which I can predict, let's say a black swan. Right.
What happens if there's a black swan? I can't predict that. Like like that's, that's outside of the bounds. So there's always a limit to given a sort of relatively static model,
what I can predict, no matter how good that model is. But we're talking about models that continuously improve.
So if I continue grabbing complexity and I continue
improving and I think, um. You'd continue rolling out, say,
your, uh, your predictions. I think you'd, um.
It would become increasingly possible for you to, to, to predict farther and farther ahead, especially if you have this
capacity to predict something that qualifies as a black swan, which is just a phase shift. Right? It's a phase shift that you can't
see at the scale at which you're generally operating your predictions. So, um, are you defining Black Swan to mean a rare single event or
like a transition to a new regime? I mean, I think it's the same,
I think a rare single event, effectively, if you do not have the
capacity to adapt to it afterwards. Is effectively a new regime. So.
Black swans can either trigger your resilience, but that effectively
means you had the capacity to deal with policies that could,
you know, take you across. So while you couldn't predict the event itself, you had policies that were capable of dealing with it.
Um, and if you can't deal with it, and you had no policies that could account for this degree of error, like this is too much.
Um, so I think that's effectively at that point you become a new regime. Interesting. Okay. Okay. Let's, um, move over to like,
general phenomenology and an activism type stuff. So, you know, um, the early inactivists, they, um,
thought that agents kind of construct their own environment and that they have a phenomenal experience. We were discussing earlier like this,
the extent to which the phenomenal component adds something to an agent.
What's your take on that? And I think it does. I think it I think it does. It it forces the agent to deal
with that information. Um. Let's say through a little more
constraints. Um, you can understand something.
But it will have very low impact on your behavior. If you don't feel it, you you can have like, you know,
rules about things, but. What if I take away the rules? Well.
There's there's nothing then like like you were just understanding doesn't entail anything. So effectively,
phenomenology sort of forces you into some constraints, right?
It it's phenomenology only arises in relation to your self-model. So.
What I'm going to focus on relates to what I need to focus on,
what will give me more information, what relates to my utility function, what I the intensity of what I feel relates to how closely or how deeply
it affects my internal states. Like there's a whole set of things like this that relate to internal external dynamics.
So I do think it adds something effectively. It adds a degree of precision relative to the policies you're
going to have to take to maintain, to self-organize, um, without these.
Um, you you just have some sort of blank picture on which you're not really capable of focusing. We're really good at this reflexive
self attention and also knowing what to pay attention to around us and knowing what level of resolution, you know, how much can we discard.
So, for example, we don't need to know what the title of all of these books are around us at the moment because it's not relevant.
So we have this great ability to just discard all relevant information. But I don't think phenomenology adds anything to that discussion,
you know, which is to say like, what is it about phenomenology? What value does it add in of itself that can't be explained by
just the basic agent framework? Well, the basic agent framework, does it have a reflexive model of itself, like is it capable of, um,
determining how it functions and how that functioning relates to specific elements outside of itself? Well, that's that's a great
Self modelling
question actually. So, um, because because we were saying earlier that to be an agent, you need to have this like
reflexive model of yourself. And in order for that to happen, you have to be around other agents because you have to pass information
out and it comes in again. So but this self-awareness, I think you would argue, only happens when you have a sufficient
level of nesting or complexity. Sure. Yeah, absolutely. But but but then how does it how does it emerge though.
Because because so so you add the complexity. But what is it that makes this reflexive self model emerge?
So we're trying to suppose right now like we're.
Trying to make a self-model emerge. Like most of the time when you create
an agent that has a self-model, you just endow it with it. So if you're asking me, how does the self-model emerge?
I think, I mean, I could explain it to you in terms of like evolutionary demands. The the kinds of systems that
we're able to, um, represent represent through some kind of tracking mechanism, their own, um, their own functioning
on top of the functioning itself. And that through that tracking
of their own functioning, they were able to modulate the precision on their functioning. I mean, that's an explanation of
how it emerges, right? On a purely evolutionary, evolutionary stance. Um, now. How does that come about?
I, I'm not sure I can tell you exactly, but we are suggesting that potentially, um, a system can track anything, right?
Can track the outside, contract the inside to some extent it can. So now that it tracks something, it makes predictions.
And these predictions are either good or they carry error. Given that they carry error too often it could choose.
It has it has the possibility to create a new state, like just just change its model, create a new state space.
And that would be structured learning. And so the the supposition would be that, let's say a system that has as
an action, as a possible action to, let's say, create a bunch of cells over here that can track that stuff, it will survive longer than that.
The system that doesn't push a bunch of cells over here that happen to have the capacity to track what they're doing. Um, so.
We were going to test this with, say, um, an agent that does a simple,
silly task, like find this thing over there, uh, or now. Now find it. But in a graph that's like a thousand node long.
Are you better at it if you have a self model or are you worse at it if you have a self model and we're not going to give it the self model,
we're going to try and see if it can through an action of modeling anything or changing anything, it can start modeling its own
processes, uh, and see if that, if that allows it to eventually, through structural learning, grow into something that has a
Anthropomorphisation
self model. Yeah. I mean, I can believe because it sounds very principled to me. So I can believe that it would
emerge and it would be useful for survival to emerge. Um, it makes a lot of sense to me. And also it reminds me of like,
you know, Daniel Dennett's intentional stance. So, like, whether or not other actors have mental states, um,
treating them as if they did, you know, so that you can make better predictions about what they do that's also very, very useful,
but also this kind of projection can be misleading because people look at language models and they say they have a theory of mind and they're,
um, regurgitating things from the psychology literature and they're saying, oh, look, you know, let's do this theory of mind tests on a language model. Oh, um,
it seems to have a theory of mind. Well, I don't think it does have a theory of mind. So, like, is it just another
form of anthropomorphization? So that's the thing. So when we say it, what do we mean? And I think it's possible.
That in the embedding because it encodes something again, that
could be sort of Turing complete. There could be something which tracks Theory of Mind, but I don't think it as in it's
having awareness of allowing it. A causal model of theory of mind
has theory of mind. That's to me is different. So can it regurgitate information that would act like theory of mind
and be as accurate as theory of mind? Pretty sure it would. Yeah, it's possible with enough data thrown at it.
It probably would. Um, but. I don't think it would be able to
tweak its own model such that it can. Really identify why and how or
where it has theory of mind. Like I don't think it could self model in that way. Uh, and I don't think it could
tweak its own model like that, whereas, um. I'm pretty sure I can.
I'm pretty sure I have the capacity to know that's what I'm doing. I'm putting that into you, and therefore through trying to
understand the causal chains that lead me to getting Theory of Mind
about you, I can maybe change how that manifests, or increment over it,
or add more precision, or even transfer it to something else, to which now I will give some degree of theory of mind,
because I've made that like conscious tweak in my model. Yeah, yeah. Okay.
Mediation and subjectivity
We had an interesting discussion earlier about mediation. So um, and it's related to semantics. Right.
So like we understand each other and in this conversation we, we are let's
say we're creating this temporary construction and we're so we're implementing the mediation pattern. And we are we've selected some
categories that we kind of agree on the semantics. And we're kind of like mediating through that shared structure.
And that to me I mean I guess that's like a kind of bespoke theory of mind which we've just created now. The fact they were capable of
mediating through language. Yeah. Not that well, I mean, as we're using language in a very broad, uh, terms, as we said earlier.
So we're communicating using language, but we are, um, using a certain vernacular, we're talking about certain categories
and so on that, that we agree with. But I guess, like the reason I'm bringing up mediation is perhaps we are, um,
communicating in a slightly adapted way than we naturally would. Yeah.
So to me, the reason that this allows me to derive that we have theory of mind is because through this I can. Sort of push away a specialization
to you that allows me no longer to compute what you're going to compute
and still together reach our goal. What this means.
I think you can model me. And I think I can model you.
That's it. To me, that's the extent of it. Like, I can put myself in your shoes and know what you're going to do,
and you can put yourself in my shoes and know what I'm going to do. And that's just shared pretensions. It's shared coordination because
I've outsourced something to you that I think reliably you're going to push towards a similar outcome. I,
I don't think it's that complicated. I think definitely language is one of our main mediums through which we understand how we reach
that coordination. But there's so much more to it. Right? Like institutions. Um. Embedded, uh, material reality,
physical constraints, phenotype time. Like, there's so much to these,
um, these shared flows that we are part of that we both know
how to interact with. And I know you know how to interact with them. And how can I recognize that? Well, I see you interacting with them.
I know that were I in your position, I would do very much the same thing.
And I can tell you can tell I'm doing the same thing because you put the chair there. I'm pretty sure you intended me
to sit and talk in that thing. You didn't tell me that we understood through the scripts that we were able to reliably
predict that this happened. So that's to me, the, the the medium is, is is quite broad, but it, it manifests through its reliability.
Yes. Yeah. Interesting. And in some sense our generative models, even though they are very, very different,
there's a significant intersection. Although I don't think it makes sense to talk about an intersection of the high resolution generative model.
It's more about the the language or not only the language. You were talking about the performativity of sitting down on a
chair and like, you know, the various the various things that that we do, um, they can create intersections to bridge understanding. Absolutely.
And that's why poetry to me is mind boggling. Like poetry to me, is precisely those intersection
which are generally inexistent. They're novel, like we're literally
creating images nobody else has created before, creating connections nobody else has created before. And yet, or at least it's the
hope people understand poetry. You have created an entirely new
constraint system, which normally doesn't point to anything.
And somehow, because there's all these webs of other constraints that
I can sort of pull from in reliable ways, you and I will create the
the new series of intersections across all these dimensions, and we
Understanding
still are capable of communicating and coordinating across that. To me, that's that's mind boggling. Yeah. Yeah.
And so subjectivity fascinates me. And poetry, we spoke about going to a music festival earlier and at the music festival, the shared
experience is that that weird sense of being with other people, clearly not under, you know, what does it mean to understand music?
What does it mean to understand poetry? I'm not sure. Like it's it's this, this moment of something new is happening here.
And we can both reliably feel the something new. And now what we make of that, something new, I think, is why poetry
is a terrible way to communicate. Like, I mean, great, you made something clever. We both felt something,
something new here. And we communicated the fact that we understand this gap. And there's the possibility for our
filling of this gap to be similar. And if it's. To me, a good poem will have reliably similar experiences in
the sense at some point mean to say something to you, not just create an experience like, um, I was just watching this, uh,
this video on YouTube the other day about Rupi Kaur and how she, with her Insta poetry, kind of ruined poetry because a lot of her
poems are like, poems are like, completely like first level. They're just they're just she just said, this is a book,
and I have a book. Like, it's like there's nothing there, right? Um. But sometimes. Through the clever ways that her
words mix in, in, in a in a new and powerful, surprising way,
you will feel this intensity and connect this intensity with, um, a terme you wouldn't have normally. And that's a new phenomenology
that has given you a meaning. You now understand what it's like to be a woman. You would never have put that
intensity on this connection of concepts that that doesn't mean anything to you. It's it's like it's no, there's no
valence there. And now there is. And suddenly you've experienced something that she meant to make you experience.
And so she has communicated what she meant to communicate. There's very low, um, uncertainty there.
But when you read a poem by some abstract poet who says something that. I mean, I it's surprising,
but I don't understand what he's trying to say to me. I feel like it's not going as far. Like, yeah, okay.
I felt the surprise, but now it has. No, it doesn't help with my model.
It doesn't go any deeper. Um, and perhaps that poetry is not for me. It doesn't. It doesn't give me a new
phenomenology. Other than the surprise of the words. Yeah. No, I that really resonates with me.
And I think where I was getting stuck before is I thought, well, and I think of meaning and understanding as being quite related,
and you could have a very impactful, phenomenal experience.
Uh, but I was worried it might stop with you, but it doesn't. So as long as the information can, um, propagate. Right.
It has this mimetic quality to it because there is no meaning without grounding. But everything is grounded.
So you're reading a poem, you're sitting in this location. It's now being grounded to this location.
You now create a yearly pilgrimage where you come back to this location with your friends, and you read the same poem,
and then this mimetic, you know, energy builds and builds and builds.
So and maybe it doesn't always work that way, but I think you can interpret that as still forming. Meaning.
No, I agree, but I think I would, I would find interesting the thought experiment of because you and I, we exist in this world, right?
So obviously our meaning is in this world, so we can create emergent structures, but in the end they always bottom out to this world.
But what if we were? To your point earlier entirely systems that only exist in words like we.
It's just words that like through some kind of functor becomes other words and then connect to other words.
And it's just a like large graph of words to words to words. And any time a path through words is enacted, it triggers
other paths through words. Um, I feel like you could have
something that resembles. Phenomenology. It just.
It just wouldn't be this. Like, there's a possibility for a system to fully be contained and self-organized within that,
so long as it's, it's, um, maintenance is conditioned upon it,
you know, having a boundary which it has to maintain. Otherwise it disappears. So I could totally see some kind
of simulation of, like, graphs that try to stay graphs and become such so complex that they maintain graphs and it wouldn't
really, um, relate to their capacity to exchange bits. Right? Again, it would have nothing to do with the physical embodiment of it.
It's just that I can't conceive of anything that wouldn't have a physical embodiment to some degree. So.
But I don't think that's impossible. Before we finish, I'm supposed to ask you about resiliency. Collective intelligence is they
Resiliency
are grown. They're not designed by definition. And they have some interesting properties.
They can kind of heal themselves, repair themselves. But there's a lot of complexity. We don't understand what they're
doing entirely. But, um, I guess like it is intuitive to me how I mean, Michael Levin talks about this,
you know, a collective intelligence is more resilient because of self-organization. But can you kind of explain from
your perspective, like why you think that is? Yeah. I mean, exactly why why is self-organization
of a group more resilient? Um, so we talked we touched upon it earlier like. How do we define resilience?
We define resilience as the capacity for a system given a perturbation to either. Remained the same. Or. Live the perturbation and return
to what it was before or. Learn something new and adapt
and become better. Right. So generally we think of black swans as these highly improbable things that you kind of you
can't really deal with. And so generally what'll happen with a black swan is it'll vastly perturb your system.
And if you have the seed of the system, it might grow again and return to what it was. So in from from that point
already it doesn't have inertia. Inertia is like the wind on the cup. The cup. The wind is a disruption,
but it's so it has such inertia in its policies that it's like it's going to stay a cup. Now a black swan is me grabbing the
cup and dropping it on the floor. It can't. It can't deal with that. Now, a plastic cup or let's say a rubber cup, it'll get on the floor,
it'll bounce a little, it'll deform and it'll come back a smart plastic cup. Now, that cup will not only, um,
you know, it'll fall, it'll learn what happened in the fall, and it'll be like, what was the call cause of the fall?
Somebody grabbed me. So next time something comes to grab me, I'll move a little bit. So that would be a plastic system.
So the reason systems that are groups have the capacity for resilience is
because they have the capacity for one, creating some core of inertia.
Something of the group will be maintained like they have a very strong inside it. It's very clear about what it is,
and it will be able to continue pushing what it is into the world. Then it has like structures that are a bit more, you know, like elastic.
They'll take they'll take disturbances, but they'll come back and they'll be able to do that because they can receive again the
signal from the inside that's like, this is what we are. Okay. All right. Let's go back. And through this, they understand
that they have predictability, potentially even redundancy. So now what happens with redundancy. Well maybe it becomes degeneracy.
So while this is doing that and I have a certainty that it's going to keep doing that because this is super certain and this is likely to return.
Now I can maybe venture off and try something different, because even
if I die, even if part of me dies, this is pretty certain to continue.
So now I have the capacity to not only should this die continue having the same function, but should this die,
return to that function, right? We both have the capacity to maintain that function where we have high certainty over this path,
which we know is a good path, but we also have the capacity to learn more and become more fit relative to a landscape and expand
such that one day when relative to this system, the black swan arrives.
But we were so big. Now we have so many paths. The Black Swans no longer going to be a black swan.
The likelihood of a black swan decreases as you increase your
fitness landscape. Mhm. Mhm. And have you studied I mean you know redundancy is quite an interesting one because
presumably the system would learn its own level of redundancy. And that must surely be related to things like its behavioral
complexity and like its sense world and environment and so on. So Alex Kiefer, so we wrote a paper on this specifically,
and Alex Kiefer wrote a beautiful formalism that basically qualified
redundancy as a degree of, um, useless complexity. Anything that is not useless complexity is degeneracy.
So I have two hands, but if I use one hand, the other isn't useless, right? It can do other things, we can play
the piano, etc. like it has the capacity to have different functions, even if it can do the same function. But if I've had five hands and
they could only do two things, I'd have like, you know, three hands too many like this. It's like it wouldn't be able unless
I could create some emergent patterns through the repetition of certain. Like at some point I would expend a lot of energy in maintaining
things which are most often useless. Interesting. But so I agree that we have to be really economical with function,
because function is our interface. But, um, you know, I was talking earlier about, you know, in this active inference framework,
we could potentially swap out agents with different skill programs so we
could have like a marketplace of intelligence, but then it's kind of degenerate to have lots of agents kicking about that we're not using,
because we're holding on to them on the basis that they might be useful in the future. So we come into a new regime.
Oh, let's bring this guy back in because it started making good predictions again. But I don't want to be holding these guys around for too long. So I think you mean redundant.
They would be redundant because they wouldn't be doing any other function. If they were degenerate, they would be trying to expand the
fitness landscape and therefore they would never be useless. Well, I would say that they are harming the the intelligence of
the overall collective. Yeah, because they are, you know, essentially they're creating predictions which are not useful.
They're polluting the predictive apparatus. Um.
But no, I think I think it's an interesting point, but but to come back to what you were saying before, so so you said if they are hanging
around, they are degeneracy. So what's the difference between redundancy and degeneracy. So that's the thing I think if
they're just hanging around and not doing anything. Yeah. Then they're just scandent. But if they're doing stuff which
is rowing in the wrong direction then it's degeneracy. Yeah. I mean, degeneracy doesn't entail that you're gonna like necessarily
go in the right direction, but it gives you the possibility to, which means you can increase your fitness landscape.
So, um, increasing your fitness landscape also means accruing error. Like you're going to make some errors and some parts of you are
going to die. So it's sometimes okay to continue accruing error. This is why I think that the
distinction between degeneracy and redundancy is important. If you're just expanding energy for something which literally doesn't
help you right now, but is doing the same thing as something else which would potentially help you. And it's not like there's so
much redundancy that, let's say, given some error.
You still wouldn't need it. Then it's probably best to pull it away. But if it's doing something else
in the meantime, even if it's rowing in the wrong direction. Yeah, it's it's still doing something.
It's still, you know, um, it's still trying to compute some error. And it's possible that over time you will find that this is just
not a thing you ever need. You will have accrued so much error that you'll just push it away. And this is how your system
changes over time as well. You don't just necessarily like, grow as a blob, right? You sometimes you just prune,
you push it away. And it is good for you to learn that you had to get rid of it. You wouldn't necessarily know
otherwise. Yeah, yeah. It's um, stimulating. Some thoughts actually, because I guess it's very similar
to Friston's idea of, of entropy. So, you know, like maintaining behavioral complexity because it might be useful.
But, um, there's this thing with gradient optimization. When you do monotonic gradient optimization, you kind of expect
to see, um, just strip away anything that's not helping me. And sometimes you need to like maintain the, the degeneracy for
a significant amount of time before it becomes useful later. So like in Friston's framework, I think it's like it's keeping
it around. Yeah. But for quite a long time. Yeah. So I think um, I think it's all about the amount of energy you have as,
say, capital. Um. And I think it touches into something
really interesting about psychology, like hoarders. Why do hoarders exist? Like if you ask them, what's the likelihood of you ever
using that thing? Like, ever at all? And they're like, well, maybe I'll need it. And then you point out to them a
situation, well, the situation happened where you would have needed it and you still didn't use it. Now what do you think?
And it just it doesn't update. So to them it's like they don't
have the right capacity to update their model such that over time,
even given that probability and the fact that it happened, etc., they can't prune it. Um, but if it doesn't cost you
anything, like you just have a, you have an adapter at home and you never go to the US, but you still have it and it's like,
well, whatever, you know, it doesn't cause that much error. Um, maybe it's worth maintaining the possibility of a policy
which would use it. It's it's it's more about, um.
Given the pool of policies you're maintaining, how much does it cost you to maintain a policy that goes all the way over there,
even though you can tell your viability is over here? You could choose to do so, and maybe it'll be adaptive.
But if it isn't, effectively the kind of system that maintains the wrong balance over the type of redundancy and degeneracy that
it maintains is just going to, yeah, it's going to perish faster than the others. Yeah, yeah. Well, this has been amazing.
Thank you so much for chatting with me. You're welcome. It was fun.